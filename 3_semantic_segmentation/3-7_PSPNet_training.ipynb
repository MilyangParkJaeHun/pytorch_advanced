{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aokMFYco5Dki"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wvHo-7O-5Dkl"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1lKxvOS65Dko"
   },
   "outputs": [],
   "source": [
    "from utils.dataloader import make_datapath_list, DataTransform, VOCDataset\n",
    "\n",
    "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
    "    rootpath=rootpath)\n",
    "\n",
    "color_mean = (0.485, 0.456, 0.406)\n",
    "color_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n",
    "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
    "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h8AkC5NH5Dkr",
    "outputId": "9d661cec-bbc4-455c-ff02-9e6639385f11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.pspnet import PSPNet\n",
    "\n",
    "net = PSPNet(n_classes=150)\n",
    "\n",
    "state_dict = torch.load(\"./weights/pspnet50_ADE20K.pth\")\n",
    "net.load_state_dict(state_dict)\n",
    "\n",
    "n_classes = 21\n",
    "net.decode_feature.classification = nn.Conv2d(\n",
    "    in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "net.aux.classification = nn.Conv2d(\n",
    "    in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:  \n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "\n",
    "net.decode_feature.classification.apply(weights_init)\n",
    "net.aux.classification.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gXuKq8VP5Dku",
    "outputId": "7257e247-3e56-4f49-e72c-8ceb68357433"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PSPNet(\n",
       "  (feature_conv): FeatureMap_convolution(\n",
       "    (cbnr_1): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (cbnr_2): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (cbnr_3): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (feature_res_1): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_res_2): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block4): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_dilated_res_1): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block4): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block5): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block6): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_dilated_res_2): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (pyramid_pooling): PyramidPooling(\n",
       "    (avpool_1): AdaptiveAvgPool2d(output_size=6)\n",
       "    (cbr_1): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (avpool_2): AdaptiveAvgPool2d(output_size=3)\n",
       "    (cbr_2): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (avpool_3): AdaptiveAvgPool2d(output_size=2)\n",
       "    (cbr_3): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (avpool_4): AdaptiveAvgPool2d(output_size=1)\n",
       "    (cbr_4): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (decode_feature): DecodePSPFeature(\n",
       "    (cbr): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (classification): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux): AuxiliaryPSPlayers(\n",
       "    (cbr): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (classification): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "C0Y6_cZh5Dkx"
   },
   "outputs": [],
   "source": [
    "class PSPLoss(nn.Module):\n",
    "    def __init__(self, aux_weight=0.4):\n",
    "        super(PSPLoss, self).__init__()\n",
    "        self.aux_weight = aux_weight \n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        loss = F.cross_entropy(outputs[0], targets, reduction='mean')\n",
    "        loss_aux = F.cross_entropy(outputs[1], targets, reduction='mean')\n",
    "\n",
    "        return loss+self.aux_weight*loss_aux\n",
    "\n",
    "\n",
    "criterion = PSPLoss(aux_weight=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1HQuH4465Dk0"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([\n",
    "    {'params': net.feature_conv.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.pyramid_pooling.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.decode_feature.parameters(), 'lr': 1e-2},\n",
    "    {'params': net.aux.parameters(), 'lr': 1e-2},\n",
    "], momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "\n",
    "def lambda_epoch(epoch):\n",
    "    max_epoch = 30\n",
    "    return math.pow((1-epoch/max_epoch), 0.9)\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QDo8kP3r5Dk3"
   },
   "outputs": [],
   "source": [
    "def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    net.to(device)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n",
    "    num_val_imgs = len(dataloaders_dict[\"val\"].dataset)\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "\n",
    "    iteration = 1\n",
    "    logs = []\n",
    "\n",
    "    # multiple minibatch\n",
    "    batch_multiplier = 3\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "        epoch_train_loss = 0.0 \n",
    "        epoch_val_loss = 0.0 \n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train() \n",
    "                scheduler.step() \n",
    "                optimizer.zero_grad()\n",
    "                print('（train）')\n",
    "\n",
    "            else:\n",
    "                if((epoch+1) % 5 == 0):\n",
    "                    net.eval()  \n",
    "                    print('-------------')\n",
    "                    print('（val）')\n",
    "                else:\n",
    "                   \n",
    "                    continue\n",
    "\n",
    "            count = 0  # multiple minibatch\n",
    "            for imges, anno_class_imges in dataloaders_dict[phase]:\n",
    "                imges = imges.to(device)\n",
    "                anno_class_imges = anno_class_imges.to(device)\n",
    "\n",
    "                \n",
    "                if (phase == 'train') and (count == 0):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    count = batch_multiplier\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(imges)\n",
    "                    loss = criterion(\n",
    "                        outputs, anno_class_imges.long()) / batch_multiplier\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  \n",
    "                        count -= 1  # multiple minibatch\n",
    "\n",
    "                        if (iteration % 10 == 0):  \n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item()/batch_size*batch_multiplier, duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item() * batch_multiplier\n",
    "                        iteration += 1\n",
    "\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item() * batch_multiplier\n",
    "\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss/num_train_imgs, epoch_val_loss/num_val_imgs))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss /\n",
    "                     num_train_imgs, 'val_loss': epoch_val_loss/num_val_imgs}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "    torch.save(net.state_dict(), 'weights/pspnet50_' +\n",
    "               str(epoch+1) + '.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1XEy-KBb5Dk5",
    "outputId": "9f1b844f-178f-45f0-80fb-b992b8b452dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "-------------\n",
      "Epoch 1/10\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 10 || Loss: 0.6948 || 10iter: 4.4443 sec.\n",
      "イテレーション 20 || Loss: 0.9359 || 10iter: 4.3896 sec.\n",
      "イテレーション 30 || Loss: 0.7485 || 10iter: 4.3678 sec.\n",
      "イテレーション 40 || Loss: 0.3689 || 10iter: 4.3875 sec.\n",
      "イテレーション 50 || Loss: 0.6341 || 10iter: 4.4141 sec.\n",
      "イテレーション 60 || Loss: 1.2326 || 10iter: 4.4320 sec.\n",
      "イテレーション 70 || Loss: 0.7648 || 10iter: 4.4270 sec.\n",
      "イテレーション 80 || Loss: 2.1241 || 10iter: 4.4097 sec.\n",
      "イテレーション 90 || Loss: 0.7412 || 10iter: 4.4047 sec.\n",
      "イテレーション 100 || Loss: 0.7777 || 10iter: 4.4201 sec.\n",
      "イテレーション 110 || Loss: 0.6610 || 10iter: 4.4061 sec.\n",
      "イテレーション 120 || Loss: 0.3256 || 10iter: 4.4099 sec.\n",
      "イテレーション 130 || Loss: 0.5868 || 10iter: 4.4124 sec.\n",
      "イテレーション 140 || Loss: 0.6622 || 10iter: 4.4034 sec.\n",
      "イテレーション 150 || Loss: 0.5399 || 10iter: 4.3953 sec.\n",
      "イテレーション 160 || Loss: 1.0238 || 10iter: 4.4094 sec.\n",
      "イテレーション 170 || Loss: 1.5876 || 10iter: 4.4161 sec.\n",
      "イテレーション 180 || Loss: 1.0056 || 10iter: 4.3985 sec.\n",
      "イテレーション 190 || Loss: 1.8141 || 10iter: 4.3936 sec.\n",
      "イテレーション 200 || Loss: 1.0705 || 10iter: 4.4064 sec.\n",
      "イテレーション 210 || Loss: 0.3629 || 10iter: 4.4017 sec.\n",
      "イテレーション 220 || Loss: 0.4177 || 10iter: 4.4002 sec.\n",
      "イテレーション 230 || Loss: 0.5764 || 10iter: 4.3914 sec.\n",
      "イテレーション 240 || Loss: 0.2599 || 10iter: 4.4015 sec.\n",
      "イテレーション 250 || Loss: 0.3458 || 10iter: 4.4174 sec.\n",
      "イテレーション 260 || Loss: 0.5824 || 10iter: 4.4356 sec.\n",
      "イテレーション 270 || Loss: 0.4356 || 10iter: 4.4168 sec.\n",
      "イテレーション 280 || Loss: 0.1801 || 10iter: 4.4431 sec.\n",
      "イテレーション 290 || Loss: 1.5065 || 10iter: 4.4216 sec.\n",
      "イテレーション 300 || Loss: 2.5018 || 10iter: 4.4134 sec.\n",
      "イテレーション 310 || Loss: 0.3382 || 10iter: 4.4229 sec.\n",
      "イテレーション 320 || Loss: 0.7646 || 10iter: 4.4283 sec.\n",
      "イテレーション 330 || Loss: 0.4533 || 10iter: 4.4227 sec.\n",
      "イテレーション 340 || Loss: 1.0464 || 10iter: 4.4228 sec.\n",
      "イテレーション 350 || Loss: 0.6926 || 10iter: 4.4124 sec.\n",
      "イテレーション 360 || Loss: 0.4672 || 10iter: 4.3952 sec.\n",
      "イテレーション 370 || Loss: 0.4771 || 10iter: 4.4110 sec.\n",
      "イテレーション 380 || Loss: 1.3243 || 10iter: 4.4113 sec.\n",
      "イテレーション 390 || Loss: 1.2305 || 10iter: 4.3980 sec.\n",
      "イテレーション 400 || Loss: 0.2440 || 10iter: 4.4028 sec.\n",
      "イテレーション 410 || Loss: 0.3863 || 10iter: 4.4075 sec.\n",
      "イテレーション 420 || Loss: 0.5617 || 10iter: 4.4053 sec.\n",
      "イテレーション 430 || Loss: 1.2781 || 10iter: 4.4359 sec.\n",
      "イテレーション 440 || Loss: 2.6319 || 10iter: 4.4177 sec.\n",
      "イテレーション 450 || Loss: 0.4278 || 10iter: 4.3813 sec.\n",
      "イテレーション 460 || Loss: 0.2768 || 10iter: 4.4087 sec.\n",
      "イテレーション 470 || Loss: 0.4198 || 10iter: 4.3887 sec.\n",
      "イテレーション 480 || Loss: 0.6322 || 10iter: 4.4165 sec.\n",
      "イテレーション 490 || Loss: 1.1304 || 10iter: 4.4135 sec.\n",
      "イテレーション 500 || Loss: 0.2991 || 10iter: 4.3804 sec.\n",
      "イテレーション 510 || Loss: 1.3721 || 10iter: 4.3886 sec.\n",
      "イテレーション 520 || Loss: 1.1352 || 10iter: 4.4027 sec.\n",
      "イテレーション 530 || Loss: 0.7489 || 10iter: 4.3909 sec.\n",
      "イテレーション 540 || Loss: 0.4681 || 10iter: 4.3934 sec.\n",
      "イテレーション 550 || Loss: 0.5261 || 10iter: 4.4060 sec.\n",
      "イテレーション 560 || Loss: 0.1871 || 10iter: 4.3852 sec.\n",
      "イテレーション 570 || Loss: 0.4797 || 10iter: 4.3926 sec.\n",
      "イテレーション 580 || Loss: 0.6919 || 10iter: 4.4243 sec.\n",
      "イテレーション 590 || Loss: 0.2523 || 10iter: 4.4241 sec.\n",
      "イテレーション 600 || Loss: 0.1468 || 10iter: 4.3983 sec.\n",
      "イテレーション 610 || Loss: 0.3465 || 10iter: 4.3953 sec.\n",
      "イテレーション 620 || Loss: 0.9638 || 10iter: 4.3959 sec.\n",
      "イテレーション 630 || Loss: 0.9093 || 10iter: 4.4090 sec.\n",
      "イテレーション 640 || Loss: 0.3546 || 10iter: 4.4063 sec.\n",
      "イテレーション 650 || Loss: 0.5031 || 10iter: 4.3929 sec.\n",
      "イテレーション 660 || Loss: 0.1323 || 10iter: 4.4124 sec.\n",
      "イテレーション 670 || Loss: 0.2550 || 10iter: 4.4196 sec.\n",
      "イテレーション 680 || Loss: 0.4003 || 10iter: 4.4249 sec.\n",
      "イテレーション 690 || Loss: 1.0308 || 10iter: 4.4377 sec.\n",
      "イテレーション 700 || Loss: 0.3138 || 10iter: 4.4375 sec.\n",
      "イテレーション 710 || Loss: 0.2102 || 10iter: 4.4228 sec.\n",
      "イテレーション 720 || Loss: 0.3535 || 10iter: 4.4185 sec.\n",
      "イテレーション 730 || Loss: 0.4914 || 10iter: 4.4155 sec.\n",
      "-------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:0.5894 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  354.1214 sec.\n",
      "-------------\n",
      "Epoch 2/10\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 740 || Loss: 0.8344 || 10iter: 3.4468 sec.\n",
      "イテレーション 750 || Loss: 0.2030 || 10iter: 4.3904 sec.\n",
      "イテレーション 760 || Loss: 0.1401 || 10iter: 4.3918 sec.\n",
      "イテレーション 770 || Loss: 0.2317 || 10iter: 4.4075 sec.\n",
      "イテレーション 780 || Loss: 0.3685 || 10iter: 4.3875 sec.\n",
      "イテレーション 790 || Loss: 0.0853 || 10iter: 4.4104 sec.\n",
      "イテレーション 800 || Loss: 0.2822 || 10iter: 4.4141 sec.\n",
      "イテレーション 810 || Loss: 0.2375 || 10iter: 4.4139 sec.\n",
      "イテレーション 820 || Loss: 0.7960 || 10iter: 4.4226 sec.\n",
      "イテレーション 830 || Loss: 0.5242 || 10iter: 4.4147 sec.\n",
      "イテレーション 840 || Loss: 0.5669 || 10iter: 4.3942 sec.\n",
      "イテレーション 850 || Loss: 0.2680 || 10iter: 4.4170 sec.\n",
      "イテレーション 860 || Loss: 0.2172 || 10iter: 4.4005 sec.\n",
      "イテレーション 870 || Loss: 0.2518 || 10iter: 4.4096 sec.\n",
      "イテレーション 880 || Loss: 0.0837 || 10iter: 4.4094 sec.\n",
      "イテレーション 890 || Loss: 0.9743 || 10iter: 4.4108 sec.\n",
      "イテレーション 900 || Loss: 0.5694 || 10iter: 4.3886 sec.\n",
      "イテレーション 910 || Loss: 0.2608 || 10iter: 4.4246 sec.\n",
      "イテレーション 920 || Loss: 0.6552 || 10iter: 4.3796 sec.\n",
      "イテレーション 930 || Loss: 0.2029 || 10iter: 4.3754 sec.\n",
      "イテレーション 940 || Loss: 0.4961 || 10iter: 4.3895 sec.\n",
      "イテレーション 950 || Loss: 0.9963 || 10iter: 4.3674 sec.\n",
      "イテレーション 960 || Loss: 0.2678 || 10iter: 4.3763 sec.\n",
      "イテレーション 970 || Loss: 0.1324 || 10iter: 4.4021 sec.\n",
      "イテレーション 980 || Loss: 0.1486 || 10iter: 4.3838 sec.\n",
      "イテレーション 990 || Loss: 0.4218 || 10iter: 4.3785 sec.\n",
      "イテレーション 1000 || Loss: 0.3089 || 10iter: 4.3888 sec.\n",
      "イテレーション 1010 || Loss: 0.1815 || 10iter: 4.3801 sec.\n",
      "イテレーション 1020 || Loss: 0.8027 || 10iter: 4.3778 sec.\n",
      "イテレーション 1030 || Loss: 0.6836 || 10iter: 4.3950 sec.\n",
      "イテレーション 1040 || Loss: 0.4441 || 10iter: 4.3802 sec.\n",
      "イテレーション 1050 || Loss: 0.3151 || 10iter: 4.3883 sec.\n",
      "イテレーション 1060 || Loss: 0.7696 || 10iter: 4.3814 sec.\n",
      "イテレーション 1070 || Loss: 0.4351 || 10iter: 4.3820 sec.\n",
      "イテレーション 1080 || Loss: 0.4543 || 10iter: 4.3763 sec.\n",
      "イテレーション 1090 || Loss: 0.1185 || 10iter: 4.3869 sec.\n",
      "イテレーション 1100 || Loss: 0.4111 || 10iter: 4.3851 sec.\n",
      "イテレーション 1110 || Loss: 0.3094 || 10iter: 4.6398 sec.\n",
      "イテレーション 1120 || Loss: 0.2443 || 10iter: 4.4031 sec.\n",
      "イテレーション 1130 || Loss: 1.1202 || 10iter: 4.5772 sec.\n",
      "イテレーション 1140 || Loss: 0.1128 || 10iter: 4.8755 sec.\n",
      "イテレーション 1150 || Loss: 0.4531 || 10iter: 5.1043 sec.\n",
      "イテレーション 1160 || Loss: 0.3565 || 10iter: 4.8031 sec.\n",
      "イテレーション 1170 || Loss: 0.2538 || 10iter: 4.6754 sec.\n",
      "イテレーション 1180 || Loss: 0.1630 || 10iter: 4.7787 sec.\n",
      "イテレーション 1190 || Loss: 0.6043 || 10iter: 4.6898 sec.\n",
      "イテレーション 1200 || Loss: 0.0757 || 10iter: 4.4722 sec.\n",
      "イテレーション 1210 || Loss: 0.2050 || 10iter: 4.4402 sec.\n",
      "イテレーション 1220 || Loss: 0.5339 || 10iter: 4.6250 sec.\n",
      "イテレーション 1230 || Loss: 0.2228 || 10iter: 4.4219 sec.\n",
      "イテレーション 1240 || Loss: 0.0890 || 10iter: 4.4466 sec.\n",
      "イテレーション 1250 || Loss: 0.7859 || 10iter: 4.5662 sec.\n",
      "イテレーション 1260 || Loss: 0.3984 || 10iter: 4.3658 sec.\n",
      "イテレーション 1270 || Loss: 0.3240 || 10iter: 4.5313 sec.\n",
      "イテレーション 1280 || Loss: 0.3603 || 10iter: 4.5205 sec.\n",
      "イテレーション 1290 || Loss: 0.6506 || 10iter: 4.5185 sec.\n",
      "イテレーション 1300 || Loss: 1.1425 || 10iter: 4.6698 sec.\n",
      "イテレーション 1310 || Loss: 0.2870 || 10iter: 4.6959 sec.\n",
      "イテレーション 1320 || Loss: 0.1972 || 10iter: 4.5775 sec.\n",
      "イテレーション 1330 || Loss: 0.2509 || 10iter: 4.8204 sec.\n",
      "イテレーション 1340 || Loss: 0.0821 || 10iter: 4.6838 sec.\n",
      "イテレーション 1350 || Loss: 0.0645 || 10iter: 4.6967 sec.\n",
      "イテレーション 1360 || Loss: 0.1508 || 10iter: 4.9336 sec.\n",
      "イテレーション 1370 || Loss: 0.8038 || 10iter: 4.6160 sec.\n",
      "イテレーション 1380 || Loss: 0.2469 || 10iter: 4.7063 sec.\n",
      "イテレーション 1390 || Loss: 0.2797 || 10iter: 4.4926 sec.\n",
      "イテレーション 1400 || Loss: 0.4235 || 10iter: 4.7317 sec.\n",
      "イテレーション 1410 || Loss: 0.4251 || 10iter: 4.7644 sec.\n",
      "イテレーション 1420 || Loss: 0.1645 || 10iter: 4.9560 sec.\n",
      "イテレーション 1430 || Loss: 0.1832 || 10iter: 4.6016 sec.\n",
      "イテレーション 1440 || Loss: 0.3460 || 10iter: 4.4195 sec.\n",
      "イテレーション 1450 || Loss: 0.3843 || 10iter: 4.4485 sec.\n",
      "イテレーション 1460 || Loss: 0.2433 || 10iter: 4.3780 sec.\n",
      "-------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:0.4100 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  362.4166 sec.\n",
      "-------------\n",
      "Epoch 3/10\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 1470 || Loss: 0.3107 || 10iter: 2.5130 sec.\n",
      "イテレーション 1480 || Loss: 0.1397 || 10iter: 4.3901 sec.\n",
      "イテレーション 1490 || Loss: 0.5004 || 10iter: 4.3732 sec.\n",
      "イテレーション 1500 || Loss: 0.5385 || 10iter: 4.3722 sec.\n",
      "イテレーション 1510 || Loss: 0.1546 || 10iter: 4.6072 sec.\n",
      "イテレーション 1520 || Loss: 0.2019 || 10iter: 4.6768 sec.\n",
      "イテレーション 1530 || Loss: 0.4491 || 10iter: 4.7786 sec.\n",
      "イテレーション 1540 || Loss: 0.9463 || 10iter: 4.6971 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イテレーション 1550 || Loss: 0.5839 || 10iter: 4.5886 sec.\n",
      "イテレーション 1560 || Loss: 0.1639 || 10iter: 4.6853 sec.\n",
      "イテレーション 1570 || Loss: 0.1787 || 10iter: 4.8270 sec.\n",
      "イテレーション 1580 || Loss: 0.1371 || 10iter: 4.6466 sec.\n",
      "イテレーション 1590 || Loss: 0.3802 || 10iter: 4.4294 sec.\n",
      "イテレーション 1600 || Loss: 0.2099 || 10iter: 4.4337 sec.\n",
      "イテレーション 1610 || Loss: 0.3987 || 10iter: 4.4281 sec.\n",
      "イテレーション 1620 || Loss: 0.5409 || 10iter: 4.4227 sec.\n",
      "イテレーション 1630 || Loss: 0.1997 || 10iter: 4.4678 sec.\n",
      "イテレーション 1640 || Loss: 0.2297 || 10iter: 4.4619 sec.\n",
      "イテレーション 1650 || Loss: 0.3531 || 10iter: 4.5183 sec.\n",
      "イテレーション 1660 || Loss: 0.5787 || 10iter: 4.4523 sec.\n",
      "イテレーション 1670 || Loss: 0.1198 || 10iter: 4.4345 sec.\n",
      "イテレーション 1680 || Loss: 0.5051 || 10iter: 4.3693 sec.\n",
      "イテレーション 1690 || Loss: 0.4206 || 10iter: 4.3698 sec.\n",
      "イテレーション 1700 || Loss: 0.2060 || 10iter: 4.5560 sec.\n",
      "イテレーション 1710 || Loss: 0.4199 || 10iter: 4.8593 sec.\n",
      "イテレーション 1720 || Loss: 0.6132 || 10iter: 4.4760 sec.\n",
      "イテレーション 1730 || Loss: 0.3721 || 10iter: 4.6602 sec.\n",
      "イテレーション 1740 || Loss: 0.3892 || 10iter: 4.7095 sec.\n",
      "イテレーション 1750 || Loss: 0.1118 || 10iter: 4.7406 sec.\n",
      "イテレーション 1760 || Loss: 0.1963 || 10iter: 4.8418 sec.\n",
      "イテレーション 1770 || Loss: 0.3873 || 10iter: 4.9025 sec.\n",
      "イテレーション 1780 || Loss: 0.1129 || 10iter: 4.9792 sec.\n",
      "イテレーション 1790 || Loss: 0.1369 || 10iter: 4.4424 sec.\n",
      "イテレーション 1800 || Loss: 0.3059 || 10iter: 4.5025 sec.\n",
      "イテレーション 1810 || Loss: 0.2539 || 10iter: 4.5970 sec.\n",
      "イテレーション 1820 || Loss: 2.8605 || 10iter: 4.3907 sec.\n",
      "イテレーション 1830 || Loss: 0.2848 || 10iter: 4.4250 sec.\n",
      "イテレーション 1840 || Loss: 0.4195 || 10iter: 4.4535 sec.\n",
      "イテレーション 1850 || Loss: 1.5375 || 10iter: 4.4299 sec.\n",
      "イテレーション 1860 || Loss: 0.2238 || 10iter: 4.4275 sec.\n",
      "イテレーション 1870 || Loss: 0.1904 || 10iter: 4.4498 sec.\n",
      "イテレーション 1880 || Loss: 0.1976 || 10iter: 4.4120 sec.\n",
      "イテレーション 1890 || Loss: 0.1999 || 10iter: 4.4328 sec.\n",
      "イテレーション 1900 || Loss: 0.2591 || 10iter: 4.4415 sec.\n",
      "イテレーション 1910 || Loss: 0.2743 || 10iter: 4.3916 sec.\n",
      "イテレーション 1920 || Loss: 0.1757 || 10iter: 4.6786 sec.\n",
      "イテレーション 1930 || Loss: 0.2406 || 10iter: 4.5955 sec.\n",
      "イテレーション 1940 || Loss: 0.0852 || 10iter: 4.4976 sec.\n",
      "イテレーション 1950 || Loss: 0.7590 || 10iter: 4.5030 sec.\n",
      "イテレーション 1960 || Loss: 0.0962 || 10iter: 4.3791 sec.\n",
      "イテレーション 1970 || Loss: 0.0633 || 10iter: 4.3806 sec.\n",
      "イテレーション 1980 || Loss: 0.2837 || 10iter: 4.3599 sec.\n",
      "イテレーション 1990 || Loss: 0.1639 || 10iter: 4.4538 sec.\n",
      "イテレーション 2000 || Loss: 0.2731 || 10iter: 4.4376 sec.\n",
      "イテレーション 2010 || Loss: 0.3594 || 10iter: 4.4351 sec.\n",
      "イテレーション 2020 || Loss: 0.5907 || 10iter: 4.4576 sec.\n",
      "イテレーション 2030 || Loss: 0.7045 || 10iter: 4.4361 sec.\n",
      "イテレーション 2040 || Loss: 0.2927 || 10iter: 4.4296 sec.\n",
      "イテレーション 2050 || Loss: 0.6456 || 10iter: 4.5452 sec.\n",
      "イテレーション 2060 || Loss: 0.9443 || 10iter: 4.5542 sec.\n",
      "イテレーション 2070 || Loss: 0.0541 || 10iter: 4.3860 sec.\n",
      "イテレーション 2080 || Loss: 0.2323 || 10iter: 4.3955 sec.\n",
      "イテレーション 2090 || Loss: 0.4108 || 10iter: 4.3955 sec.\n",
      "イテレーション 2100 || Loss: 1.2596 || 10iter: 4.4717 sec.\n",
      "イテレーション 2110 || Loss: 0.5394 || 10iter: 4.4870 sec.\n",
      "イテレーション 2120 || Loss: 0.2109 || 10iter: 4.4799 sec.\n",
      "イテレーション 2130 || Loss: 0.3661 || 10iter: 4.4767 sec.\n",
      "イテレーション 2140 || Loss: 0.2766 || 10iter: 4.4343 sec.\n",
      "イテレーション 2150 || Loss: 0.0773 || 10iter: 4.4044 sec.\n",
      "イテレーション 2160 || Loss: 0.4839 || 10iter: 4.4600 sec.\n",
      "イテレーション 2170 || Loss: 0.6208 || 10iter: 4.4616 sec.\n",
      "イテレーション 2180 || Loss: 0.9976 || 10iter: 4.4098 sec.\n",
      "イテレーション 2190 || Loss: 0.1390 || 10iter: 4.5566 sec.\n",
      "-------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:0.3660 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  363.2671 sec.\n",
      "-------------\n",
      "Epoch 4/10\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2200 || Loss: 0.5358 || 10iter: 1.5862 sec.\n",
      "イテレーション 2210 || Loss: 0.5534 || 10iter: 4.5779 sec.\n",
      "イテレーション 2220 || Loss: 0.1811 || 10iter: 4.7881 sec.\n",
      "イテレーション 2230 || Loss: 0.2682 || 10iter: 4.7087 sec.\n",
      "イテレーション 2240 || Loss: 0.1291 || 10iter: 4.5616 sec.\n",
      "イテレーション 2250 || Loss: 0.4317 || 10iter: 4.7702 sec.\n",
      "イテレーション 2260 || Loss: 0.1745 || 10iter: 4.6593 sec.\n",
      "イテレーション 2270 || Loss: 0.1988 || 10iter: 4.6872 sec.\n",
      "イテレーション 2280 || Loss: 0.1388 || 10iter: 4.6704 sec.\n",
      "イテレーション 2290 || Loss: 0.7004 || 10iter: 4.9927 sec.\n",
      "イテレーション 2300 || Loss: 0.2946 || 10iter: 4.7156 sec.\n",
      "イテレーション 2310 || Loss: 0.2697 || 10iter: 4.9959 sec.\n",
      "イテレーション 2320 || Loss: 0.2636 || 10iter: 4.9028 sec.\n",
      "イテレーション 2330 || Loss: 0.4125 || 10iter: 4.9302 sec.\n",
      "イテレーション 2340 || Loss: 0.1542 || 10iter: 4.4664 sec.\n",
      "イテレーション 2350 || Loss: 0.2201 || 10iter: 4.6871 sec.\n",
      "イテレーション 2360 || Loss: 0.7173 || 10iter: 4.4643 sec.\n",
      "イテレーション 2370 || Loss: 0.3321 || 10iter: 4.4366 sec.\n",
      "イテレーション 2380 || Loss: 0.2849 || 10iter: 4.4465 sec.\n",
      "イテレーション 2390 || Loss: 0.3768 || 10iter: 4.4283 sec.\n",
      "イテレーション 2400 || Loss: 0.2664 || 10iter: 4.6096 sec.\n",
      "イテレーション 2410 || Loss: 0.0754 || 10iter: 4.4392 sec.\n",
      "イテレーション 2420 || Loss: 0.1824 || 10iter: 4.5996 sec.\n",
      "イテレーション 2430 || Loss: 0.7876 || 10iter: 4.6947 sec.\n",
      "イテレーション 2440 || Loss: 0.3243 || 10iter: 4.4514 sec.\n",
      "イテレーション 2450 || Loss: 0.9152 || 10iter: 4.6854 sec.\n",
      "イテレーション 2460 || Loss: 0.1998 || 10iter: 4.4884 sec.\n",
      "イテレーション 2470 || Loss: 0.0781 || 10iter: 4.4004 sec.\n",
      "イテレーション 2480 || Loss: 0.1012 || 10iter: 4.3935 sec.\n",
      "イテレーション 2490 || Loss: 0.2916 || 10iter: 4.5151 sec.\n",
      "イテレーション 2500 || Loss: 0.6996 || 10iter: 4.4742 sec.\n",
      "イテレーション 2510 || Loss: 0.1720 || 10iter: 4.3686 sec.\n",
      "イテレーション 2520 || Loss: 1.0098 || 10iter: 4.3866 sec.\n",
      "イテレーション 2530 || Loss: 0.1820 || 10iter: 4.5466 sec.\n",
      "イテレーション 2540 || Loss: 0.5131 || 10iter: 4.4065 sec.\n",
      "イテレーション 2550 || Loss: 0.3954 || 10iter: 4.3825 sec.\n",
      "イテレーション 2560 || Loss: 0.2607 || 10iter: 4.3805 sec.\n",
      "イテレーション 2570 || Loss: 0.1034 || 10iter: 4.3698 sec.\n",
      "イテレーション 2580 || Loss: 0.4252 || 10iter: 4.3882 sec.\n",
      "イテレーション 2590 || Loss: 0.2461 || 10iter: 4.3726 sec.\n",
      "イテレーション 2600 || Loss: 0.2574 || 10iter: 4.3863 sec.\n",
      "イテレーション 2610 || Loss: 0.1432 || 10iter: 4.3825 sec.\n",
      "イテレーション 2620 || Loss: 0.4216 || 10iter: 4.3884 sec.\n",
      "イテレーション 2630 || Loss: 1.2195 || 10iter: 4.3913 sec.\n",
      "イテレーション 2640 || Loss: 0.2622 || 10iter: 4.3933 sec.\n",
      "イテレーション 2650 || Loss: 0.6973 || 10iter: 4.4658 sec.\n",
      "イテレーション 2660 || Loss: 0.6247 || 10iter: 4.4132 sec.\n",
      "イテレーション 2670 || Loss: 0.1324 || 10iter: 4.5546 sec.\n",
      "イテレーション 2680 || Loss: 0.6558 || 10iter: 4.4922 sec.\n",
      "イテレーション 2690 || Loss: 0.3982 || 10iter: 4.5893 sec.\n",
      "イテレーション 2700 || Loss: 0.2020 || 10iter: 4.6089 sec.\n",
      "イテレーション 2710 || Loss: 0.2190 || 10iter: 4.5364 sec.\n",
      "イテレーション 2720 || Loss: 0.1376 || 10iter: 4.3865 sec.\n",
      "イテレーション 2730 || Loss: 0.1763 || 10iter: 4.3825 sec.\n",
      "イテレーション 2740 || Loss: 0.1979 || 10iter: 4.3937 sec.\n",
      "イテレーション 2750 || Loss: 0.1370 || 10iter: 4.3695 sec.\n",
      "イテレーション 2760 || Loss: 0.0804 || 10iter: 4.3815 sec.\n",
      "イテレーション 2770 || Loss: 0.1938 || 10iter: 4.3866 sec.\n",
      "イテレーション 2780 || Loss: 0.9287 || 10iter: 4.3784 sec.\n",
      "イテレーション 2790 || Loss: 0.2174 || 10iter: 4.3675 sec.\n",
      "イテレーション 2800 || Loss: 0.5319 || 10iter: 4.3689 sec.\n",
      "イテレーション 2810 || Loss: 0.4112 || 10iter: 4.3707 sec.\n",
      "イテレーション 2820 || Loss: 0.1841 || 10iter: 4.3637 sec.\n",
      "イテレーション 2830 || Loss: 0.2519 || 10iter: 4.3789 sec.\n",
      "イテレーション 2840 || Loss: 0.1873 || 10iter: 4.3641 sec.\n",
      "イテレーション 2850 || Loss: 0.4113 || 10iter: 4.3592 sec.\n",
      "イテレーション 2860 || Loss: 0.3126 || 10iter: 4.5955 sec.\n",
      "イテレーション 2870 || Loss: 0.1735 || 10iter: 4.6138 sec.\n",
      "イテレーション 2880 || Loss: 0.2712 || 10iter: 4.6742 sec.\n",
      "イテレーション 2890 || Loss: 0.2548 || 10iter: 4.7787 sec.\n",
      "イテレーション 2900 || Loss: 0.1529 || 10iter: 4.9345 sec.\n",
      "イテレーション 2910 || Loss: 0.1388 || 10iter: 4.5205 sec.\n",
      "イテレーション 2920 || Loss: 0.8047 || 10iter: 4.8833 sec.\n",
      "-------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:0.3302 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  363.9441 sec.\n",
      "-------------\n",
      "Epoch 5/10\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 2930 || Loss: 0.1409 || 10iter: 0.5810 sec.\n",
      "イテレーション 2940 || Loss: 0.0914 || 10iter: 4.5992 sec.\n",
      "イテレーション 2950 || Loss: 0.3267 || 10iter: 4.7552 sec.\n",
      "イテレーション 2960 || Loss: 0.3567 || 10iter: 4.6324 sec.\n",
      "イテレーション 2970 || Loss: 0.2674 || 10iter: 4.8824 sec.\n",
      "イテレーション 2980 || Loss: 0.5534 || 10iter: 4.6478 sec.\n",
      "イテレーション 2990 || Loss: 0.2528 || 10iter: 4.5531 sec.\n",
      "イテレーション 3000 || Loss: 0.0972 || 10iter: 4.7165 sec.\n",
      "イテレーション 3010 || Loss: 0.2788 || 10iter: 4.5055 sec.\n",
      "イテレーション 3020 || Loss: 0.5148 || 10iter: 4.3955 sec.\n",
      "イテレーション 3030 || Loss: 1.0204 || 10iter: 4.7433 sec.\n",
      "イテレーション 3040 || Loss: 0.7306 || 10iter: 4.9215 sec.\n",
      "イテレーション 3050 || Loss: 0.1806 || 10iter: 4.8403 sec.\n",
      "イテレーション 3060 || Loss: 0.1895 || 10iter: 4.6561 sec.\n",
      "イテレーション 3070 || Loss: 0.4964 || 10iter: 4.8004 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イテレーション 3080 || Loss: 0.0716 || 10iter: 4.4733 sec.\n",
      "イテレーション 3090 || Loss: 0.1096 || 10iter: 4.7482 sec.\n",
      "イテレーション 3100 || Loss: 0.3142 || 10iter: 4.9526 sec.\n",
      "イテレーション 3110 || Loss: 0.2835 || 10iter: 4.9331 sec.\n",
      "イテレーション 3120 || Loss: 0.0475 || 10iter: 4.8389 sec.\n",
      "イテレーション 3130 || Loss: 0.1662 || 10iter: 4.7421 sec.\n",
      "イテレーション 3140 || Loss: 0.1982 || 10iter: 4.6679 sec.\n",
      "イテレーション 3150 || Loss: 0.0477 || 10iter: 4.5491 sec.\n",
      "イテレーション 3160 || Loss: 0.1411 || 10iter: 4.5116 sec.\n",
      "イテレーション 3170 || Loss: 0.1201 || 10iter: 4.9884 sec.\n",
      "イテレーション 3180 || Loss: 0.1363 || 10iter: 4.4951 sec.\n",
      "イテレーション 3190 || Loss: 0.1348 || 10iter: 4.7632 sec.\n",
      "イテレーション 3200 || Loss: 0.3434 || 10iter: 4.7782 sec.\n",
      "イテレーション 3210 || Loss: 0.0806 || 10iter: 4.7595 sec.\n",
      "イテレーション 3220 || Loss: 0.0807 || 10iter: 4.9220 sec.\n",
      "イテレーション 3230 || Loss: 0.0486 || 10iter: 4.6077 sec.\n",
      "イテレーション 3240 || Loss: 0.2553 || 10iter: 4.4889 sec.\n",
      "イテレーション 3250 || Loss: 0.1463 || 10iter: 4.5164 sec.\n",
      "イテレーション 3260 || Loss: 0.2423 || 10iter: 4.5235 sec.\n",
      "イテレーション 3270 || Loss: 0.1001 || 10iter: 4.5045 sec.\n",
      "イテレーション 3280 || Loss: 0.4052 || 10iter: 4.4696 sec.\n",
      "イテレーション 3290 || Loss: 0.3639 || 10iter: 4.7249 sec.\n",
      "イテレーション 3300 || Loss: 0.2111 || 10iter: 4.3913 sec.\n",
      "イテレーション 3310 || Loss: 0.1986 || 10iter: 4.4569 sec.\n",
      "イテレーション 3320 || Loss: 0.1290 || 10iter: 4.4075 sec.\n",
      "イテレーション 3330 || Loss: 0.3746 || 10iter: 4.3977 sec.\n",
      "イテレーション 3340 || Loss: 0.0787 || 10iter: 4.4002 sec.\n",
      "イテレーション 3350 || Loss: 0.0897 || 10iter: 4.5108 sec.\n",
      "イテレーション 3360 || Loss: 0.6420 || 10iter: 4.5689 sec.\n",
      "イテレーション 3370 || Loss: 0.2968 || 10iter: 4.4639 sec.\n",
      "イテレーション 3380 || Loss: 0.1654 || 10iter: 4.4249 sec.\n",
      "イテレーション 3390 || Loss: 0.9024 || 10iter: 4.5941 sec.\n",
      "イテレーション 3400 || Loss: 0.0524 || 10iter: 4.8251 sec.\n",
      "イテレーション 3410 || Loss: 0.1555 || 10iter: 4.5001 sec.\n",
      "イテレーション 3420 || Loss: 0.0982 || 10iter: 4.4627 sec.\n",
      "イテレーション 3430 || Loss: 0.7027 || 10iter: 4.5019 sec.\n",
      "イテレーション 3440 || Loss: 0.4970 || 10iter: 4.5033 sec.\n",
      "イテレーション 3450 || Loss: 0.4317 || 10iter: 4.5316 sec.\n",
      "イテレーション 3460 || Loss: 0.1777 || 10iter: 4.8375 sec.\n",
      "イテレーション 3470 || Loss: 0.2227 || 10iter: 4.4713 sec.\n",
      "イテレーション 3480 || Loss: 0.6238 || 10iter: 4.4342 sec.\n",
      "イテレーション 3490 || Loss: 0.3057 || 10iter: 4.4812 sec.\n",
      "イテレーション 3500 || Loss: 0.1558 || 10iter: 4.4184 sec.\n",
      "イテレーション 3510 || Loss: 0.3802 || 10iter: 4.5848 sec.\n",
      "イテレーション 3520 || Loss: 0.0879 || 10iter: 5.0540 sec.\n",
      "イテレーション 3530 || Loss: 0.2078 || 10iter: 4.6287 sec.\n",
      "イテレーション 3540 || Loss: 0.0783 || 10iter: 4.5214 sec.\n",
      "イテレーション 3550 || Loss: 0.1130 || 10iter: 4.6276 sec.\n",
      "イテレーション 3560 || Loss: 0.3677 || 10iter: 4.6138 sec.\n",
      "イテレーション 3570 || Loss: 0.3385 || 10iter: 4.5415 sec.\n",
      "イテレーション 3580 || Loss: 0.1302 || 10iter: 4.4772 sec.\n",
      "イテレーション 3590 || Loss: 0.3143 || 10iter: 4.4044 sec.\n",
      "イテレーション 3600 || Loss: 0.5518 || 10iter: 4.4929 sec.\n",
      "イテレーション 3610 || Loss: 0.1755 || 10iter: 4.4170 sec.\n",
      "イテレーション 3620 || Loss: 0.4552 || 10iter: 4.3840 sec.\n",
      "イテレーション 3630 || Loss: 0.1569 || 10iter: 4.4188 sec.\n",
      "イテレーション 3640 || Loss: 0.2595 || 10iter: 4.3863 sec.\n",
      "イテレーション 3650 || Loss: 0.0636 || 10iter: 4.4187 sec.\n",
      "イテレーション 3660 || Loss: 0.0730 || 10iter: 4.4409 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 5 || Epoch_TRAIN_Loss:0.3220 ||Epoch_VAL_Loss:0.3617\n",
      "timer:  496.9252 sec.\n",
      "-------------\n",
      "Epoch 6/10\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 3670 || Loss: 0.0752 || 10iter: 4.4664 sec.\n",
      "イテレーション 3680 || Loss: 0.5183 || 10iter: 4.4107 sec.\n",
      "イテレーション 3690 || Loss: 0.1369 || 10iter: 4.4191 sec.\n",
      "イテレーション 3700 || Loss: 1.1868 || 10iter: 4.4138 sec.\n",
      "イテレーション 3710 || Loss: 0.0759 || 10iter: 4.4340 sec.\n",
      "イテレーション 3720 || Loss: 0.1192 || 10iter: 4.5653 sec.\n",
      "イテレーション 3730 || Loss: 0.1276 || 10iter: 4.7540 sec.\n",
      "イテレーション 3740 || Loss: 0.0987 || 10iter: 4.5249 sec.\n",
      "イテレーション 3750 || Loss: 0.3309 || 10iter: 4.5089 sec.\n",
      "イテレーション 3760 || Loss: 0.7504 || 10iter: 4.6038 sec.\n",
      "イテレーション 3770 || Loss: 0.2674 || 10iter: 4.4596 sec.\n",
      "イテレーション 3780 || Loss: 0.2688 || 10iter: 4.4285 sec.\n",
      "イテレーション 3790 || Loss: 0.1036 || 10iter: 4.4483 sec.\n",
      "イテレーション 3800 || Loss: 0.4033 || 10iter: 4.7918 sec.\n",
      "イテレーション 3810 || Loss: 0.2349 || 10iter: 4.6761 sec.\n",
      "イテレーション 3820 || Loss: 0.3015 || 10iter: 4.6776 sec.\n",
      "イテレーション 3830 || Loss: 0.3082 || 10iter: 4.6040 sec.\n",
      "イテレーション 3840 || Loss: 0.4383 || 10iter: 4.5196 sec.\n",
      "イテレーション 3850 || Loss: 0.2433 || 10iter: 4.4569 sec.\n",
      "イテレーション 3860 || Loss: 0.4396 || 10iter: 4.5308 sec.\n",
      "イテレーション 3870 || Loss: 0.3563 || 10iter: 4.7592 sec.\n",
      "イテレーション 3880 || Loss: 0.4262 || 10iter: 4.6068 sec.\n",
      "イテレーション 3890 || Loss: 0.5297 || 10iter: 4.6874 sec.\n",
      "イテレーション 3900 || Loss: 0.7796 || 10iter: 4.5089 sec.\n",
      "イテレーション 3910 || Loss: 0.1610 || 10iter: 4.8744 sec.\n",
      "イテレーション 3920 || Loss: 0.2196 || 10iter: 4.8657 sec.\n",
      "イテレーション 3930 || Loss: 0.1139 || 10iter: 4.6809 sec.\n",
      "イテレーション 3940 || Loss: 0.0765 || 10iter: 4.8117 sec.\n",
      "イテレーション 3950 || Loss: 0.1225 || 10iter: 4.4748 sec.\n",
      "イテレーション 3960 || Loss: 0.5311 || 10iter: 4.7621 sec.\n",
      "イテレーション 3970 || Loss: 0.1736 || 10iter: 4.4509 sec.\n",
      "イテレーション 3980 || Loss: 0.9169 || 10iter: 4.5077 sec.\n",
      "イテレーション 3990 || Loss: 0.5215 || 10iter: 4.3959 sec.\n",
      "イテレーション 4000 || Loss: 0.1638 || 10iter: 4.4024 sec.\n",
      "イテレーション 4010 || Loss: 0.4480 || 10iter: 4.4254 sec.\n",
      "イテレーション 4020 || Loss: 0.1129 || 10iter: 4.3980 sec.\n",
      "イテレーション 4030 || Loss: 0.5724 || 10iter: 4.4569 sec.\n",
      "イテレーション 4040 || Loss: 0.1443 || 10iter: 4.6665 sec.\n",
      "イテレーション 4050 || Loss: 0.2116 || 10iter: 4.7027 sec.\n",
      "イテレーション 4060 || Loss: 0.5641 || 10iter: 4.7363 sec.\n",
      "イテレーション 4070 || Loss: 0.5753 || 10iter: 4.8932 sec.\n",
      "イテレーション 4080 || Loss: 0.0358 || 10iter: 4.8739 sec.\n",
      "イテレーション 4090 || Loss: 0.0574 || 10iter: 4.5307 sec.\n",
      "イテレーション 4100 || Loss: 1.4088 || 10iter: 4.4006 sec.\n",
      "イテレーション 4110 || Loss: 0.2805 || 10iter: 4.4110 sec.\n",
      "イテレーション 4120 || Loss: 0.0596 || 10iter: 4.3960 sec.\n",
      "イテレーション 4130 || Loss: 0.0832 || 10iter: 4.3927 sec.\n",
      "イテレーション 4140 || Loss: 0.1555 || 10iter: 4.4064 sec.\n",
      "イテレーション 4150 || Loss: 0.6368 || 10iter: 4.5083 sec.\n",
      "イテレーション 4160 || Loss: 0.1395 || 10iter: 5.0049 sec.\n",
      "イテレーション 4170 || Loss: 0.5746 || 10iter: 5.2856 sec.\n",
      "イテレーション 4180 || Loss: 0.6099 || 10iter: 5.1493 sec.\n",
      "イテレーション 4190 || Loss: 0.2786 || 10iter: 5.1561 sec.\n",
      "イテレーション 4200 || Loss: 0.2937 || 10iter: 4.6189 sec.\n",
      "イテレーション 4210 || Loss: 0.3591 || 10iter: 4.5833 sec.\n",
      "イテレーション 4220 || Loss: 0.2883 || 10iter: 4.4907 sec.\n",
      "イテレーション 4230 || Loss: 0.2256 || 10iter: 4.5929 sec.\n",
      "イテレーション 4240 || Loss: 0.0992 || 10iter: 4.4055 sec.\n",
      "イテレーション 4250 || Loss: 0.3660 || 10iter: 4.3960 sec.\n",
      "イテレーション 4260 || Loss: 0.2733 || 10iter: 4.4983 sec.\n",
      "イテレーション 4270 || Loss: 1.3979 || 10iter: 4.4145 sec.\n",
      "イテレーション 4280 || Loss: 0.2409 || 10iter: 4.3940 sec.\n",
      "イテレーション 4290 || Loss: 0.1718 || 10iter: 4.3957 sec.\n",
      "イテレーション 4300 || Loss: 0.3080 || 10iter: 4.5291 sec.\n",
      "イテレーション 4310 || Loss: 0.2441 || 10iter: 4.3678 sec.\n",
      "イテレーション 4320 || Loss: 0.1014 || 10iter: 4.3996 sec.\n",
      "イテレーション 4330 || Loss: 0.0923 || 10iter: 4.7664 sec.\n",
      "イテレーション 4340 || Loss: 0.4180 || 10iter: 4.8293 sec.\n",
      "イテレーション 4350 || Loss: 0.2735 || 10iter: 4.8032 sec.\n",
      "イテレーション 4360 || Loss: 0.1482 || 10iter: 4.8418 sec.\n",
      "イテレーション 4370 || Loss: 0.3028 || 10iter: 4.8652 sec.\n",
      "イテレーション 4380 || Loss: 0.2205 || 10iter: 4.6377 sec.\n",
      "イテレーション 4390 || Loss: 0.2171 || 10iter: 4.9259 sec.\n",
      "-------------\n",
      "epoch 6 || Epoch_TRAIN_Loss:0.3046 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  369.9586 sec.\n",
      "-------------\n",
      "Epoch 7/10\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 4400 || Loss: 0.1405 || 10iter: 3.8751 sec.\n",
      "イテレーション 4410 || Loss: 0.2146 || 10iter: 4.9109 sec.\n",
      "イテレーション 4420 || Loss: 0.2589 || 10iter: 4.9378 sec.\n",
      "イテレーション 4430 || Loss: 1.0422 || 10iter: 4.6960 sec.\n",
      "イテレーション 4440 || Loss: 0.1684 || 10iter: 4.6552 sec.\n",
      "イテレーション 4450 || Loss: 0.2044 || 10iter: 4.9412 sec.\n",
      "イテレーション 4460 || Loss: 0.0504 || 10iter: 4.4239 sec.\n",
      "イテレーション 4470 || Loss: 0.3086 || 10iter: 4.4007 sec.\n",
      "イテレーション 4480 || Loss: 0.3258 || 10iter: 4.4017 sec.\n",
      "イテレーション 4490 || Loss: 0.0652 || 10iter: 4.4143 sec.\n",
      "イテレーション 4500 || Loss: 0.0727 || 10iter: 4.5147 sec.\n",
      "イテレーション 4510 || Loss: 0.1943 || 10iter: 4.5810 sec.\n",
      "イテレーション 4520 || Loss: 0.2610 || 10iter: 4.4850 sec.\n",
      "イテレーション 4530 || Loss: 0.6336 || 10iter: 4.5114 sec.\n",
      "イテレーション 4540 || Loss: 0.2082 || 10iter: 4.5588 sec.\n",
      "イテレーション 4550 || Loss: 0.1055 || 10iter: 4.5201 sec.\n",
      "イテレーション 4560 || Loss: 0.2384 || 10iter: 4.5107 sec.\n",
      "イテレーション 4570 || Loss: 0.4992 || 10iter: 4.6283 sec.\n",
      "イテレーション 4580 || Loss: 0.1338 || 10iter: 4.5237 sec.\n",
      "イテレーション 4590 || Loss: 0.1526 || 10iter: 4.4218 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イテレーション 4600 || Loss: 0.2000 || 10iter: 4.5443 sec.\n",
      "イテレーション 4610 || Loss: 0.3828 || 10iter: 4.4555 sec.\n",
      "イテレーション 4620 || Loss: 0.2563 || 10iter: 4.4374 sec.\n",
      "イテレーション 4630 || Loss: 0.1371 || 10iter: 4.4287 sec.\n",
      "イテレーション 4640 || Loss: 0.1559 || 10iter: 4.4152 sec.\n",
      "イテレーション 4650 || Loss: 0.2454 || 10iter: 4.5476 sec.\n",
      "イテレーション 4660 || Loss: 0.2435 || 10iter: 4.4613 sec.\n",
      "イテレーション 4670 || Loss: 0.1341 || 10iter: 4.4628 sec.\n",
      "イテレーション 4680 || Loss: 0.4345 || 10iter: 4.4175 sec.\n",
      "イテレーション 4690 || Loss: 0.1245 || 10iter: 4.7475 sec.\n",
      "イテレーション 4700 || Loss: 0.1939 || 10iter: 4.6605 sec.\n",
      "イテレーション 4710 || Loss: 0.1539 || 10iter: 4.5614 sec.\n",
      "イテレーション 4720 || Loss: 0.1455 || 10iter: 4.5555 sec.\n",
      "イテレーション 4730 || Loss: 0.0974 || 10iter: 4.4801 sec.\n",
      "イテレーション 4740 || Loss: 0.2430 || 10iter: 4.4672 sec.\n",
      "イテレーション 4750 || Loss: 0.1010 || 10iter: 4.5228 sec.\n",
      "イテレーション 4760 || Loss: 0.2322 || 10iter: 4.8028 sec.\n",
      "イテレーション 4770 || Loss: 0.5388 || 10iter: 4.8771 sec.\n",
      "イテレーション 4780 || Loss: 0.2587 || 10iter: 4.5162 sec.\n",
      "イテレーション 4790 || Loss: 0.3074 || 10iter: 4.6443 sec.\n",
      "イテレーション 4800 || Loss: 0.3305 || 10iter: 4.5710 sec.\n",
      "イテレーション 4810 || Loss: 0.2411 || 10iter: 4.5193 sec.\n",
      "イテレーション 4820 || Loss: 0.3400 || 10iter: 4.4799 sec.\n",
      "イテレーション 4830 || Loss: 0.0570 || 10iter: 4.4262 sec.\n",
      "イテレーション 4840 || Loss: 0.4263 || 10iter: 4.4370 sec.\n",
      "イテレーション 4850 || Loss: 0.7192 || 10iter: 4.4403 sec.\n",
      "イテレーション 4860 || Loss: 0.5701 || 10iter: 4.4382 sec.\n",
      "イテレーション 4870 || Loss: 0.4242 || 10iter: 4.4117 sec.\n",
      "イテレーション 4880 || Loss: 2.2391 || 10iter: 4.5162 sec.\n",
      "イテレーション 4890 || Loss: 0.1297 || 10iter: 4.5071 sec.\n",
      "イテレーション 4900 || Loss: 0.1155 || 10iter: 4.8094 sec.\n",
      "イテレーション 4910 || Loss: 0.2953 || 10iter: 4.8096 sec.\n",
      "イテレーション 4920 || Loss: 0.0857 || 10iter: 4.7912 sec.\n",
      "イテレーション 4930 || Loss: 0.1379 || 10iter: 5.0849 sec.\n",
      "イテレーション 4940 || Loss: 0.0256 || 10iter: 5.1234 sec.\n",
      "イテレーション 4950 || Loss: 0.0982 || 10iter: 4.6940 sec.\n",
      "イテレーション 4960 || Loss: 0.3601 || 10iter: 4.5410 sec.\n",
      "イテレーション 4970 || Loss: 0.9591 || 10iter: 4.9776 sec.\n",
      "イテレーション 4980 || Loss: 0.1624 || 10iter: 4.6244 sec.\n",
      "イテレーション 4990 || Loss: 0.1222 || 10iter: 4.6065 sec.\n",
      "イテレーション 5000 || Loss: 0.1281 || 10iter: 4.5085 sec.\n",
      "イテレーション 5010 || Loss: 0.4093 || 10iter: 4.5727 sec.\n",
      "イテレーション 5020 || Loss: 1.2581 || 10iter: 4.5116 sec.\n",
      "イテレーション 5030 || Loss: 0.0555 || 10iter: 4.5207 sec.\n",
      "イテレーション 5040 || Loss: 0.2248 || 10iter: 4.6209 sec.\n",
      "イテレーション 5050 || Loss: 0.2442 || 10iter: 4.7053 sec.\n",
      "イテレーション 5060 || Loss: 0.1156 || 10iter: 4.9566 sec.\n",
      "イテレーション 5070 || Loss: 0.1814 || 10iter: 4.7804 sec.\n",
      "イテレーション 5080 || Loss: 0.1951 || 10iter: 4.6765 sec.\n",
      "イテレーション 5090 || Loss: 0.2543 || 10iter: 4.8924 sec.\n",
      "イテレーション 5100 || Loss: 0.1318 || 10iter: 4.5749 sec.\n",
      "イテレーション 5110 || Loss: 0.2310 || 10iter: 4.8167 sec.\n",
      "イテレーション 5120 || Loss: 0.2079 || 10iter: 4.5653 sec.\n",
      "-------------\n",
      "epoch 7 || Epoch_TRAIN_Loss:0.2849 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  370.2154 sec.\n",
      "-------------\n",
      "Epoch 8/10\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 5130 || Loss: 0.2537 || 10iter: 2.5297 sec.\n",
      "イテレーション 5140 || Loss: 0.1524 || 10iter: 5.0949 sec.\n",
      "イテレーション 5150 || Loss: 0.2362 || 10iter: 4.8442 sec.\n",
      "イテレーション 5160 || Loss: 0.7723 || 10iter: 4.4442 sec.\n",
      "イテレーション 5170 || Loss: 0.4073 || 10iter: 4.6422 sec.\n",
      "イテレーション 5180 || Loss: 0.0744 || 10iter: 4.5054 sec.\n",
      "イテレーション 5190 || Loss: 0.1221 || 10iter: 4.4539 sec.\n",
      "イテレーション 5200 || Loss: 0.4412 || 10iter: 4.5700 sec.\n",
      "イテレーション 5210 || Loss: 0.1134 || 10iter: 4.4983 sec.\n",
      "イテレーション 5220 || Loss: 0.1402 || 10iter: 4.4712 sec.\n",
      "イテレーション 5230 || Loss: 0.2229 || 10iter: 4.5011 sec.\n",
      "イテレーション 5240 || Loss: 0.1209 || 10iter: 4.4471 sec.\n",
      "イテレーション 5250 || Loss: 0.4138 || 10iter: 4.4677 sec.\n",
      "イテレーション 5260 || Loss: 0.4772 || 10iter: 4.6381 sec.\n",
      "イテレーション 5270 || Loss: 0.2288 || 10iter: 4.5851 sec.\n",
      "イテレーション 5280 || Loss: 0.1180 || 10iter: 4.4417 sec.\n",
      "イテレーション 5290 || Loss: 1.2859 || 10iter: 4.4933 sec.\n",
      "イテレーション 5300 || Loss: 0.1540 || 10iter: 4.6814 sec.\n",
      "イテレーション 5310 || Loss: 0.2994 || 10iter: 4.7556 sec.\n",
      "イテレーション 5320 || Loss: 1.3092 || 10iter: 4.8640 sec.\n",
      "イテレーション 5330 || Loss: 0.2334 || 10iter: 4.5764 sec.\n",
      "イテレーション 5340 || Loss: 0.1817 || 10iter: 4.6528 sec.\n",
      "イテレーション 5350 || Loss: 0.0734 || 10iter: 4.8439 sec.\n",
      "イテレーション 5360 || Loss: 0.5262 || 10iter: 4.8798 sec.\n",
      "イテレーション 5370 || Loss: 0.2929 || 10iter: 4.5319 sec.\n",
      "イテレーション 5380 || Loss: 0.2563 || 10iter: 4.4682 sec.\n",
      "イテレーション 5390 || Loss: 0.3818 || 10iter: 4.4219 sec.\n",
      "イテレーション 5400 || Loss: 0.0527 || 10iter: 4.4257 sec.\n",
      "イテレーション 5410 || Loss: 0.4527 || 10iter: 4.4367 sec.\n",
      "イテレーション 5420 || Loss: 0.2720 || 10iter: 4.4302 sec.\n",
      "イテレーション 5430 || Loss: 0.0965 || 10iter: 4.6794 sec.\n",
      "イテレーション 5440 || Loss: 0.2927 || 10iter: 4.5724 sec.\n",
      "イテレーション 5450 || Loss: 0.1489 || 10iter: 4.4614 sec.\n",
      "イテレーション 5460 || Loss: 0.0762 || 10iter: 4.5671 sec.\n",
      "イテレーション 5470 || Loss: 0.1575 || 10iter: 4.8177 sec.\n",
      "イテレーション 5480 || Loss: 0.1529 || 10iter: 4.8650 sec.\n",
      "イテレーション 5490 || Loss: 0.3686 || 10iter: 4.4298 sec.\n",
      "イテレーション 5500 || Loss: 0.1030 || 10iter: 4.4614 sec.\n",
      "イテレーション 5510 || Loss: 0.7982 || 10iter: 4.6157 sec.\n",
      "イテレーション 5520 || Loss: 0.4811 || 10iter: 4.5787 sec.\n",
      "イテレーション 5530 || Loss: 0.3339 || 10iter: 4.6141 sec.\n",
      "イテレーション 5540 || Loss: 0.2413 || 10iter: 4.4972 sec.\n",
      "イテレーション 5550 || Loss: 0.1480 || 10iter: 4.7782 sec.\n",
      "イテレーション 5560 || Loss: 0.1228 || 10iter: 4.4988 sec.\n",
      "イテレーション 5570 || Loss: 0.1781 || 10iter: 4.4149 sec.\n",
      "イテレーション 5580 || Loss: 0.3093 || 10iter: 4.4147 sec.\n",
      "イテレーション 5590 || Loss: 0.3845 || 10iter: 4.4192 sec.\n",
      "イテレーション 5600 || Loss: 0.1948 || 10iter: 4.5943 sec.\n",
      "イテレーション 5610 || Loss: 0.2935 || 10iter: 4.5779 sec.\n",
      "イテレーション 5620 || Loss: 0.2704 || 10iter: 4.4144 sec.\n",
      "イテレーション 5630 || Loss: 0.2267 || 10iter: 4.3942 sec.\n",
      "イテレーション 5640 || Loss: 0.3364 || 10iter: 4.5040 sec.\n",
      "イテレーション 5650 || Loss: 0.0559 || 10iter: 4.9253 sec.\n",
      "イテレーション 5660 || Loss: 0.2097 || 10iter: 4.7285 sec.\n",
      "イテレーション 5670 || Loss: 0.2768 || 10iter: 4.4850 sec.\n",
      "イテレーション 5680 || Loss: 0.4655 || 10iter: 4.6334 sec.\n",
      "イテレーション 5690 || Loss: 0.2381 || 10iter: 5.0704 sec.\n",
      "イテレーション 5700 || Loss: 0.3774 || 10iter: 4.7525 sec.\n",
      "イテレーション 5710 || Loss: 0.2673 || 10iter: 4.7814 sec.\n",
      "イテレーション 5720 || Loss: 0.1854 || 10iter: 4.7060 sec.\n",
      "イテレーション 5730 || Loss: 0.2295 || 10iter: 4.9037 sec.\n",
      "イテレーション 5740 || Loss: 0.0989 || 10iter: 4.6926 sec.\n",
      "イテレーション 5750 || Loss: 0.0654 || 10iter: 4.4867 sec.\n",
      "イテレーション 5760 || Loss: 0.1922 || 10iter: 4.7199 sec.\n",
      "イテレーション 5770 || Loss: 0.1503 || 10iter: 4.7939 sec.\n",
      "イテレーション 5780 || Loss: 0.2672 || 10iter: 4.7652 sec.\n",
      "イテレーション 5790 || Loss: 0.0957 || 10iter: 4.5767 sec.\n",
      "イテレーション 5800 || Loss: 0.1401 || 10iter: 4.6026 sec.\n",
      "イテレーション 5810 || Loss: 0.1762 || 10iter: 4.7884 sec.\n",
      "イテレーション 5820 || Loss: 0.1596 || 10iter: 4.7336 sec.\n",
      "イテレーション 5830 || Loss: 0.7916 || 10iter: 4.7870 sec.\n",
      "イテレーション 5840 || Loss: 0.2500 || 10iter: 4.7386 sec.\n",
      "イテレーション 5850 || Loss: 0.1385 || 10iter: 4.6954 sec.\n",
      "-------------\n",
      "epoch 8 || Epoch_TRAIN_Loss:0.2724 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  371.2970 sec.\n",
      "-------------\n",
      "Epoch 9/10\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 5860 || Loss: 0.9309 || 10iter: 1.6439 sec.\n",
      "イテレーション 5870 || Loss: 0.1180 || 10iter: 4.6089 sec.\n",
      "イテレーション 5880 || Loss: 0.1708 || 10iter: 4.5398 sec.\n",
      "イテレーション 5890 || Loss: 0.2883 || 10iter: 4.4523 sec.\n",
      "イテレーション 5900 || Loss: 0.1331 || 10iter: 4.7107 sec.\n",
      "イテレーション 5910 || Loss: 0.0850 || 10iter: 4.7817 sec.\n",
      "イテレーション 5920 || Loss: 0.0587 || 10iter: 4.5384 sec.\n",
      "イテレーション 5930 || Loss: 0.1649 || 10iter: 4.7324 sec.\n",
      "イテレーション 5940 || Loss: 0.1050 || 10iter: 4.5048 sec.\n",
      "イテレーション 5950 || Loss: 0.1199 || 10iter: 4.6451 sec.\n",
      "イテレーション 5960 || Loss: 0.3686 || 10iter: 4.6653 sec.\n",
      "イテレーション 5970 || Loss: 2.4706 || 10iter: 4.6570 sec.\n",
      "イテレーション 5980 || Loss: 0.1267 || 10iter: 4.5588 sec.\n",
      "イテレーション 5990 || Loss: 0.0956 || 10iter: 4.5086 sec.\n",
      "イテレーション 6000 || Loss: 0.3061 || 10iter: 4.6291 sec.\n",
      "イテレーション 6010 || Loss: 0.5489 || 10iter: 4.7348 sec.\n",
      "イテレーション 6020 || Loss: 0.2875 || 10iter: 4.5727 sec.\n",
      "イテレーション 6030 || Loss: 0.6315 || 10iter: 4.5832 sec.\n",
      "イテレーション 6040 || Loss: 0.5121 || 10iter: 4.4661 sec.\n",
      "イテレーション 6050 || Loss: 0.2469 || 10iter: 4.3896 sec.\n",
      "イテレーション 6060 || Loss: 0.2568 || 10iter: 4.4015 sec.\n",
      "イテレーション 6070 || Loss: 0.2570 || 10iter: 4.4006 sec.\n",
      "イテレーション 6080 || Loss: 0.1029 || 10iter: 4.4102 sec.\n",
      "イテレーション 6090 || Loss: 0.4768 || 10iter: 4.4197 sec.\n",
      "イテレーション 6100 || Loss: 0.1185 || 10iter: 4.4066 sec.\n",
      "イテレーション 6110 || Loss: 0.0730 || 10iter: 4.4853 sec.\n",
      "イテレーション 6120 || Loss: 0.0927 || 10iter: 4.4250 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イテレーション 6130 || Loss: 0.1819 || 10iter: 4.4153 sec.\n",
      "イテレーション 6140 || Loss: 0.0835 || 10iter: 4.4013 sec.\n",
      "イテレーション 6150 || Loss: 0.3021 || 10iter: 4.3880 sec.\n",
      "イテレーション 6160 || Loss: 0.2175 || 10iter: 4.4093 sec.\n",
      "イテレーション 6170 || Loss: 0.1171 || 10iter: 4.3882 sec.\n",
      "イテレーション 6180 || Loss: 0.2212 || 10iter: 4.4155 sec.\n",
      "イテレーション 6190 || Loss: 0.3209 || 10iter: 4.4378 sec.\n",
      "イテレーション 6200 || Loss: 0.0742 || 10iter: 4.4278 sec.\n",
      "イテレーション 6210 || Loss: 0.0353 || 10iter: 4.4123 sec.\n",
      "イテレーション 6220 || Loss: 0.2231 || 10iter: 4.3871 sec.\n",
      "イテレーション 6230 || Loss: 0.1280 || 10iter: 4.4098 sec.\n",
      "イテレーション 6240 || Loss: 1.7717 || 10iter: 4.4073 sec.\n",
      "イテレーション 6250 || Loss: 0.0757 || 10iter: 4.3895 sec.\n",
      "イテレーション 6260 || Loss: 0.1733 || 10iter: 4.3915 sec.\n",
      "イテレーション 6270 || Loss: 0.9794 || 10iter: 4.4025 sec.\n",
      "イテレーション 6280 || Loss: 0.2921 || 10iter: 4.3962 sec.\n",
      "イテレーション 6290 || Loss: 0.1226 || 10iter: 4.3931 sec.\n",
      "イテレーション 6300 || Loss: 0.3200 || 10iter: 4.4095 sec.\n",
      "イテレーション 6310 || Loss: 0.4412 || 10iter: 4.3944 sec.\n",
      "イテレーション 6320 || Loss: 1.5442 || 10iter: 4.3867 sec.\n",
      "イテレーション 6330 || Loss: 0.0912 || 10iter: 4.3922 sec.\n",
      "イテレーション 6340 || Loss: 0.1536 || 10iter: 4.3904 sec.\n",
      "イテレーション 6350 || Loss: 0.1275 || 10iter: 4.3826 sec.\n",
      "イテレーション 6360 || Loss: 0.1815 || 10iter: 4.3807 sec.\n",
      "イテレーション 6370 || Loss: 0.1722 || 10iter: 4.3917 sec.\n",
      "イテレーション 6380 || Loss: 0.2805 || 10iter: 4.3931 sec.\n",
      "イテレーション 6390 || Loss: 0.3402 || 10iter: 4.4197 sec.\n",
      "イテレーション 6400 || Loss: 0.4499 || 10iter: 4.4108 sec.\n",
      "イテレーション 6410 || Loss: 0.6920 || 10iter: 4.4046 sec.\n",
      "イテレーション 6420 || Loss: 0.1204 || 10iter: 4.4154 sec.\n",
      "イテレーション 6430 || Loss: 0.1009 || 10iter: 4.4025 sec.\n",
      "イテレーション 6440 || Loss: 0.0468 || 10iter: 4.3928 sec.\n",
      "イテレーション 6450 || Loss: 0.3292 || 10iter: 4.3866 sec.\n",
      "イテレーション 6460 || Loss: 0.1527 || 10iter: 4.4000 sec.\n",
      "イテレーション 6470 || Loss: 0.4883 || 10iter: 4.3742 sec.\n",
      "イテレーション 6480 || Loss: 0.4198 || 10iter: 4.3951 sec.\n",
      "イテレーション 6490 || Loss: 0.1066 || 10iter: 4.3877 sec.\n",
      "イテレーション 6500 || Loss: 0.1426 || 10iter: 4.4020 sec.\n",
      "イテレーション 6510 || Loss: 0.2874 || 10iter: 4.4181 sec.\n",
      "イテレーション 6520 || Loss: 0.1602 || 10iter: 4.3930 sec.\n",
      "イテレーション 6530 || Loss: 0.1164 || 10iter: 4.3907 sec.\n",
      "イテレーション 6540 || Loss: 0.5664 || 10iter: 4.3874 sec.\n",
      "イテレーション 6550 || Loss: 0.5086 || 10iter: 4.3921 sec.\n",
      "イテレーション 6560 || Loss: 0.1100 || 10iter: 4.3635 sec.\n",
      "イテレーション 6570 || Loss: 0.1003 || 10iter: 4.3878 sec.\n",
      "イテレーション 6580 || Loss: 0.4331 || 10iter: 4.3899 sec.\n",
      "-------------\n",
      "epoch 9 || Epoch_TRAIN_Loss:0.2632 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  357.8660 sec.\n",
      "-------------\n",
      "Epoch 10/10\n",
      "-------------\n",
      "（train）\n",
      "イテレーション 6590 || Loss: 0.3416 || 10iter: 0.5403 sec.\n",
      "イテレーション 6600 || Loss: 0.0956 || 10iter: 4.3911 sec.\n",
      "イテレーション 6610 || Loss: 0.6050 || 10iter: 4.3833 sec.\n",
      "イテレーション 6620 || Loss: 0.3410 || 10iter: 4.3672 sec.\n",
      "イテレーション 6630 || Loss: 0.1191 || 10iter: 4.4008 sec.\n",
      "イテレーション 6640 || Loss: 0.1757 || 10iter: 4.3851 sec.\n",
      "イテレーション 6650 || Loss: 0.3217 || 10iter: 4.3965 sec.\n",
      "イテレーション 6660 || Loss: 0.3038 || 10iter: 4.3941 sec.\n",
      "イテレーション 6670 || Loss: 0.3875 || 10iter: 4.3931 sec.\n",
      "イテレーション 6680 || Loss: 0.0833 || 10iter: 4.3886 sec.\n",
      "イテレーション 6690 || Loss: 0.1463 || 10iter: 4.3920 sec.\n",
      "イテレーション 6700 || Loss: 0.1906 || 10iter: 4.3897 sec.\n",
      "イテレーション 6710 || Loss: 0.3622 || 10iter: 4.3884 sec.\n",
      "イテレーション 6720 || Loss: 0.1358 || 10iter: 4.3883 sec.\n",
      "イテレーション 6730 || Loss: 0.1629 || 10iter: 4.4004 sec.\n",
      "イテレーション 6740 || Loss: 0.2450 || 10iter: 4.3899 sec.\n",
      "イテレーション 6750 || Loss: 0.1883 || 10iter: 4.3880 sec.\n",
      "イテレーション 6760 || Loss: 0.1472 || 10iter: 4.3932 sec.\n",
      "イテレーション 6770 || Loss: 0.3072 || 10iter: 4.4079 sec.\n",
      "イテレーション 6780 || Loss: 0.3149 || 10iter: 4.3924 sec.\n",
      "イテレーション 6790 || Loss: 0.0484 || 10iter: 4.4021 sec.\n",
      "イテレーション 6800 || Loss: 0.2123 || 10iter: 4.3906 sec.\n",
      "イテレーション 6810 || Loss: 0.1054 || 10iter: 4.3917 sec.\n",
      "イテレーション 6820 || Loss: 0.8332 || 10iter: 4.3957 sec.\n",
      "イテレーション 6830 || Loss: 0.1989 || 10iter: 4.3891 sec.\n",
      "イテレーション 6840 || Loss: 0.2140 || 10iter: 4.3845 sec.\n",
      "イテレーション 6850 || Loss: 0.4505 || 10iter: 4.4075 sec.\n",
      "イテレーション 6860 || Loss: 0.1588 || 10iter: 4.4042 sec.\n",
      "イテレーション 6870 || Loss: 0.2416 || 10iter: 4.3717 sec.\n",
      "イテレーション 6880 || Loss: 0.1790 || 10iter: 4.3995 sec.\n",
      "イテレーション 6890 || Loss: 0.1279 || 10iter: 4.3832 sec.\n",
      "イテレーション 6900 || Loss: 0.3595 || 10iter: 4.3738 sec.\n",
      "イテレーション 6910 || Loss: 0.1334 || 10iter: 4.3972 sec.\n",
      "イテレーション 6920 || Loss: 0.0876 || 10iter: 4.3858 sec.\n",
      "イテレーション 6930 || Loss: 0.1965 || 10iter: 4.3958 sec.\n",
      "イテレーション 6940 || Loss: 0.2322 || 10iter: 4.3883 sec.\n",
      "イテレーション 6950 || Loss: 1.5163 || 10iter: 4.3805 sec.\n",
      "イテレーション 6960 || Loss: 0.2690 || 10iter: 4.3911 sec.\n",
      "イテレーション 6970 || Loss: 0.1271 || 10iter: 4.3903 sec.\n",
      "イテレーション 6980 || Loss: 0.2269 || 10iter: 4.3687 sec.\n",
      "イテレーション 6990 || Loss: 0.4718 || 10iter: 4.3966 sec.\n",
      "イテレーション 7000 || Loss: 0.4243 || 10iter: 4.3993 sec.\n",
      "イテレーション 7010 || Loss: 0.0302 || 10iter: 4.4134 sec.\n",
      "イテレーション 7020 || Loss: 0.1129 || 10iter: 4.3941 sec.\n",
      "イテレーション 7030 || Loss: 0.1529 || 10iter: 4.3986 sec.\n",
      "イテレーション 7040 || Loss: 0.0967 || 10iter: 4.3647 sec.\n",
      "イテレーション 7050 || Loss: 0.0925 || 10iter: 4.3712 sec.\n",
      "イテレーション 7060 || Loss: 0.2142 || 10iter: 4.3997 sec.\n",
      "イテレーション 7070 || Loss: 0.2002 || 10iter: 4.3796 sec.\n",
      "イテレーション 7080 || Loss: 0.3738 || 10iter: 4.3741 sec.\n",
      "イテレーション 7090 || Loss: 0.5405 || 10iter: 4.3743 sec.\n",
      "イテレーション 7100 || Loss: 0.2169 || 10iter: 4.3948 sec.\n",
      "イテレーション 7110 || Loss: 0.0667 || 10iter: 4.3789 sec.\n",
      "イテレーション 7120 || Loss: 0.1728 || 10iter: 4.3927 sec.\n",
      "イテレーション 7130 || Loss: 0.3635 || 10iter: 4.3734 sec.\n",
      "イテレーション 7140 || Loss: 0.1989 || 10iter: 4.3827 sec.\n",
      "イテレーション 7150 || Loss: 0.2882 || 10iter: 4.3927 sec.\n",
      "イテレーション 7160 || Loss: 0.2167 || 10iter: 4.3899 sec.\n",
      "イテレーション 7170 || Loss: 0.2428 || 10iter: 4.3989 sec.\n",
      "イテレーション 7180 || Loss: 0.1456 || 10iter: 4.3728 sec.\n",
      "イテレーション 7190 || Loss: 0.2860 || 10iter: 4.3733 sec.\n",
      "イテレーション 7200 || Loss: 0.2562 || 10iter: 4.3840 sec.\n",
      "イテレーション 7210 || Loss: 0.1264 || 10iter: 4.3873 sec.\n",
      "イテレーション 7220 || Loss: 0.7003 || 10iter: 4.3837 sec.\n",
      "イテレーション 7230 || Loss: 0.0792 || 10iter: 4.3939 sec.\n",
      "イテレーション 7240 || Loss: 0.0987 || 10iter: 4.3776 sec.\n",
      "イテレーション 7250 || Loss: 0.1259 || 10iter: 4.3855 sec.\n",
      "イテレーション 7260 || Loss: 0.1686 || 10iter: 4.3789 sec.\n",
      "イテレーション 7270 || Loss: 0.3210 || 10iter: 4.4110 sec.\n",
      "イテレーション 7280 || Loss: 0.4389 || 10iter: 4.3983 sec.\n",
      "イテレーション 7290 || Loss: 0.1170 || 10iter: 4.3983 sec.\n",
      "イテレーション 7300 || Loss: 0.5623 || 10iter: 4.4113 sec.\n",
      "イテレーション 7310 || Loss: 0.2626 || 10iter: 4.4143 sec.\n",
      "イテレーション 7320 || Loss: 0.2262 || 10iter: 4.4024 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 10 || Epoch_TRAIN_Loss:0.2348 ||Epoch_VAL_Loss:0.3569\n",
      "timer:  475.4371 sec.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3-7_PSPNet_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
