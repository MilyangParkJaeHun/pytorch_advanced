{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aokMFYco5Dki"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wvHo-7O-5Dkl"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1lKxvOS65Dko"
   },
   "outputs": [],
   "source": [
    "from utils.dataloader import make_datapath_list, DataTransform, VOCDataset\n",
    "\n",
    "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
    "    rootpath=rootpath)\n",
    "\n",
    "color_mean = (0.485, 0.456, 0.406)\n",
    "color_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n",
    "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
    "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h8AkC5NH5Dkr",
    "outputId": "9d661cec-bbc4-455c-ff02-9e6639385f11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.pspnet import PSPNet\n",
    "\n",
    "net = PSPNet(n_classes=150)\n",
    "\n",
    "state_dict = torch.load(\"./weights/pspnet50_ADE20K.pth\")\n",
    "net.load_state_dict(state_dict)\n",
    "\n",
    "# 분류용 합성곱 층을 출력 수 21로 바꾼다.\n",
    "n_classes = 21\n",
    "net.decode_feature.classification = nn.Conv2d(\n",
    "    in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "net.aux.classification = nn.Conv2d(\n",
    "    in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "# 교체한 합성곱 층 초기화, 활성화 함수는 시그모이드 함수이므로 Xavier 사용\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:  \n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "\n",
    "net.decode_feature.classification.apply(weights_init)\n",
    "net.aux.classification.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gXuKq8VP5Dku",
    "outputId": "7257e247-3e56-4f49-e72c-8ceb68357433"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PSPNet(\n",
       "  (feature_conv): FeatureMap_convolution(\n",
       "    (cbnr_1): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (cbnr_2): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (cbnr_3): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (feature_res_1): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_res_2): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block4): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_dilated_res_1): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block4): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block5): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block6): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_dilated_res_2): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (pyramid_pooling): PyramidPooling(\n",
       "    (avpool_1): AdaptiveAvgPool2d(output_size=6)\n",
       "    (cbr_1): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (avpool_2): AdaptiveAvgPool2d(output_size=3)\n",
       "    (cbr_2): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (avpool_3): AdaptiveAvgPool2d(output_size=2)\n",
       "    (cbr_3): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (avpool_4): AdaptiveAvgPool2d(output_size=1)\n",
       "    (cbr_4): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (decode_feature): DecodePSPFeature(\n",
       "    (cbr): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (classification): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux): AuxiliaryPSPlayers(\n",
       "    (cbr): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (classification): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "C0Y6_cZh5Dkx"
   },
   "outputs": [],
   "source": [
    "class PSPLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, aux_weight=0.4):\n",
    "        super(PSPLoss, self).__init__()\n",
    "        self.aux_weight = aux_weight \n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        loss = F.cross_entropy(outputs[0], targets, reduction='mean')\n",
    "        loss_aux = F.cross_entropy(outputs[1], targets, reduction='mean')\n",
    "\n",
    "        return loss+self.aux_weight*loss_aux\n",
    "\n",
    "\n",
    "criterion = PSPLoss(aux_weight=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1HQuH4465Dk0"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([\n",
    "    {'params': net.feature_conv.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.pyramid_pooling.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.decode_feature.parameters(), 'lr': 1e-2},\n",
    "    {'params': net.aux.parameters(), 'lr': 1e-2},\n",
    "], momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "\n",
    "def lambda_epoch(epoch):\n",
    "    max_epoch = 30\n",
    "    return math.pow((1-epoch/max_epoch), 0.9)\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QDo8kP3r5Dk3"
   },
   "outputs": [],
   "source": [
    "def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    net.to(device)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n",
    "    num_val_imgs = len(dataloaders_dict[\"val\"].dataset)\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "\n",
    "    # 반복자의 카운터 설정\n",
    "    iteration = 1\n",
    "    logs = []\n",
    "\n",
    "    # multiple minibatch\n",
    "    batch_multiplier = 3\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "        epoch_train_loss = 0.0 \n",
    "        epoch_val_loss = 0.0 \n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # 에폭별 훈련 및 검증 루프\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train() # 모델을 훈련 모드로\n",
    "                scheduler.step() # 최적화 스케줄러 갱신\n",
    "                optimizer.zero_grad()\n",
    "                print('（train）')\n",
    "\n",
    "            else:\n",
    "                if((epoch+1) % 5 == 0):\n",
    "                    net.eval() # 모델을 검증 모드로\n",
    "                    print('-------------')\n",
    "                    print('（val）')\n",
    "                else:\n",
    "                   \n",
    "                    continue\n",
    "\n",
    "            count = 0  # multiple minibatch\n",
    "            for imges, anno_class_imges in dataloaders_dict[phase]:\n",
    "                # 미니 배치 크기가 1이면 배치 정규화에서 오류가 발생하여 피한다.\n",
    "                if imges.size()[0] == 1:\n",
    "                    continue\n",
    "                    \n",
    "                imges = imges.to(device)\n",
    "                anno_class_imges = anno_class_imges.to(device)\n",
    "    \n",
    "                # 멀티플 미니 배치로 파라미터 갱신\n",
    "                if (phase == 'train') and (count == 0):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    count = batch_multiplier\n",
    "\n",
    "                # 순전파 계산\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(imges)\n",
    "                    loss = criterion(\n",
    "                        outputs, anno_class_imges.long()) / batch_multiplier\n",
    "\n",
    "                    # 훈련 시에는 역전파\n",
    "                    if phase == 'train':\n",
    "                        loss.backward() # 경사 계산 \n",
    "                        count -= 1  # multiple minibatch\n",
    "\n",
    "                        if (iteration % 10 == 0):  \n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('{} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item()/batch_size*batch_multiplier, duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item() * batch_multiplier\n",
    "                        iteration += 1\n",
    "\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item() * batch_multiplier\n",
    "\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss/num_train_imgs, epoch_val_loss/num_val_imgs))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss /\n",
    "                     num_train_imgs, 'val_loss': epoch_val_loss/num_val_imgs}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "    torch.save(net.state_dict(), 'weights/pspnet50_' +\n",
    "               str(epoch+1) + '.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1XEy-KBb5Dk5",
    "outputId": "9f1b844f-178f-45f0-80fb-b992b8b452dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "-------------\n",
      "Epoch 1/30\n",
      "-------------\n",
      "（train）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 || Loss: 0.2051 || 10iter: 24.2783 sec.\n",
      "20 || Loss: 0.1111 || 10iter: 13.8646 sec.\n",
      "30 || Loss: 0.1252 || 10iter: 13.9891 sec.\n",
      "40 || Loss: 0.1190 || 10iter: 14.0360 sec.\n",
      "50 || Loss: 0.0668 || 10iter: 14.0472 sec.\n",
      "60 || Loss: 0.0866 || 10iter: 14.1550 sec.\n",
      "70 || Loss: 0.0907 || 10iter: 14.0014 sec.\n",
      "80 || Loss: 0.1052 || 10iter: 14.0917 sec.\n",
      "90 || Loss: 0.0534 || 10iter: 14.1339 sec.\n",
      "-------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:0.1205 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  158.6779 sec.\n",
      "-------------\n",
      "Epoch 2/30\n",
      "-------------\n",
      "（train）\n",
      "100 || Loss: 0.0785 || 10iter: 10.9121 sec.\n",
      "110 || Loss: 0.0738 || 10iter: 13.9288 sec.\n",
      "120 || Loss: 0.0459 || 10iter: 13.9713 sec.\n",
      "130 || Loss: 0.0619 || 10iter: 13.9901 sec.\n",
      "140 || Loss: 0.0632 || 10iter: 13.9975 sec.\n",
      "150 || Loss: 0.0478 || 10iter: 14.0216 sec.\n",
      "160 || Loss: 0.0893 || 10iter: 14.0104 sec.\n",
      "170 || Loss: 0.0533 || 10iter: 13.9985 sec.\n",
      "180 || Loss: 0.0452 || 10iter: 14.0042 sec.\n",
      "-------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:0.0538 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  140.2822 sec.\n",
      "-------------\n",
      "Epoch 3/30\n",
      "-------------\n",
      "（train）\n",
      "190 || Loss: 0.0398 || 10iter: 9.0486 sec.\n",
      "200 || Loss: 0.0470 || 10iter: 16.0688 sec.\n",
      "210 || Loss: 0.0471 || 10iter: 16.0965 sec.\n",
      "220 || Loss: 0.0303 || 10iter: 16.0940 sec.\n",
      "230 || Loss: 0.0601 || 10iter: 16.1138 sec.\n",
      "240 || Loss: 0.0402 || 10iter: 16.1071 sec.\n",
      "250 || Loss: 0.0437 || 10iter: 16.1226 sec.\n",
      "260 || Loss: 0.0420 || 10iter: 16.1210 sec.\n",
      "270 || Loss: 0.0304 || 10iter: 16.0970 sec.\n",
      "-------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:0.0430 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.6660 sec.\n",
      "-------------\n",
      "Epoch 4/30\n",
      "-------------\n",
      "（train）\n",
      "280 || Loss: 0.0486 || 10iter: 5.5119 sec.\n",
      "290 || Loss: 0.0384 || 10iter: 16.1073 sec.\n",
      "300 || Loss: 0.0306 || 10iter: 16.1291 sec.\n",
      "310 || Loss: 0.0292 || 10iter: 16.1204 sec.\n",
      "320 || Loss: 0.0331 || 10iter: 16.1144 sec.\n",
      "330 || Loss: 0.0369 || 10iter: 16.1385 sec.\n",
      "340 || Loss: 0.0372 || 10iter: 16.1440 sec.\n",
      "350 || Loss: 0.0324 || 10iter: 16.1232 sec.\n",
      "360 || Loss: 0.0239 || 10iter: 16.1177 sec.\n",
      "-------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:0.0385 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.8683 sec.\n",
      "-------------\n",
      "Epoch 5/30\n",
      "-------------\n",
      "（train）\n",
      "370 || Loss: 0.0401 || 10iter: 1.9670 sec.\n",
      "380 || Loss: 0.0279 || 10iter: 16.1378 sec.\n",
      "390 || Loss: 0.0225 || 10iter: 16.1515 sec.\n",
      "400 || Loss: 0.0279 || 10iter: 16.1441 sec.\n",
      "410 || Loss: 0.0455 || 10iter: 16.1325 sec.\n",
      "420 || Loss: 0.0271 || 10iter: 16.1324 sec.\n",
      "430 || Loss: 0.0345 || 10iter: 16.1159 sec.\n",
      "440 || Loss: 0.0454 || 10iter: 16.1317 sec.\n",
      "450 || Loss: 0.0284 || 10iter: 16.1467 sec.\n",
      "460 || Loss: 0.0560 || 10iter: 15.9944 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 5 || Epoch_TRAIN_Loss:0.0357 ||Epoch_VAL_Loss:0.0438\n",
      "timer:  208.1528 sec.\n",
      "-------------\n",
      "Epoch 6/30\n",
      "-------------\n",
      "（train）\n",
      "470 || Loss: 0.0246 || 10iter: 16.1619 sec.\n",
      "480 || Loss: 0.0285 || 10iter: 16.1096 sec.\n",
      "490 || Loss: 0.0304 || 10iter: 16.1418 sec.\n",
      "500 || Loss: 0.0246 || 10iter: 16.1193 sec.\n",
      "510 || Loss: 0.0455 || 10iter: 16.1260 sec.\n",
      "520 || Loss: 0.0245 || 10iter: 16.1263 sec.\n",
      "530 || Loss: 0.0374 || 10iter: 16.1282 sec.\n",
      "540 || Loss: 0.0355 || 10iter: 16.1257 sec.\n",
      "550 || Loss: 0.0501 || 10iter: 16.1235 sec.\n",
      "-------------\n",
      "epoch 6 || Epoch_TRAIN_Loss:0.0343 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.8937 sec.\n",
      "-------------\n",
      "Epoch 7/30\n",
      "-------------\n",
      "（train）\n",
      "560 || Loss: 0.0323 || 10iter: 12.6184 sec.\n",
      "570 || Loss: 0.0289 || 10iter: 16.1418 sec.\n",
      "580 || Loss: 0.0352 || 10iter: 16.1167 sec.\n",
      "590 || Loss: 0.0299 || 10iter: 16.1236 sec.\n",
      "600 || Loss: 0.0321 || 10iter: 16.1269 sec.\n",
      "610 || Loss: 0.0205 || 10iter: 16.1425 sec.\n",
      "620 || Loss: 0.0352 || 10iter: 16.1052 sec.\n",
      "630 || Loss: 0.0273 || 10iter: 16.1175 sec.\n",
      "640 || Loss: 0.0381 || 10iter: 16.1343 sec.\n",
      "-------------\n",
      "epoch 7 || Epoch_TRAIN_Loss:0.0330 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.9115 sec.\n",
      "-------------\n",
      "Epoch 8/30\n",
      "-------------\n",
      "（train）\n",
      "650 || Loss: 0.0194 || 10iter: 9.0408 sec.\n",
      "660 || Loss: 0.0217 || 10iter: 16.1120 sec.\n",
      "670 || Loss: 0.0317 || 10iter: 16.1362 sec.\n",
      "680 || Loss: 0.0379 || 10iter: 16.1296 sec.\n",
      "690 || Loss: 0.0320 || 10iter: 16.1519 sec.\n",
      "700 || Loss: 0.0332 || 10iter: 16.1035 sec.\n",
      "710 || Loss: 0.0277 || 10iter: 16.1153 sec.\n",
      "720 || Loss: 0.0416 || 10iter: 16.1645 sec.\n",
      "730 || Loss: 0.0339 || 10iter: 16.1532 sec.\n",
      "-------------\n",
      "epoch 8 || Epoch_TRAIN_Loss:0.0313 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.9093 sec.\n",
      "-------------\n",
      "Epoch 9/30\n",
      "-------------\n",
      "（train）\n",
      "740 || Loss: 0.0380 || 10iter: 5.5215 sec.\n",
      "750 || Loss: 0.0325 || 10iter: 16.1145 sec.\n",
      "760 || Loss: 0.0208 || 10iter: 16.1754 sec.\n",
      "770 || Loss: 0.0301 || 10iter: 16.1220 sec.\n",
      "780 || Loss: 0.0385 || 10iter: 16.1283 sec.\n",
      "790 || Loss: 0.0316 || 10iter: 16.1353 sec.\n",
      "800 || Loss: 0.0247 || 10iter: 16.1419 sec.\n",
      "810 || Loss: 0.0335 || 10iter: 16.1300 sec.\n",
      "820 || Loss: 0.0273 || 10iter: 16.1480 sec.\n",
      "-------------\n",
      "epoch 9 || Epoch_TRAIN_Loss:0.0309 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.9830 sec.\n",
      "-------------\n",
      "Epoch 10/30\n",
      "-------------\n",
      "（train）\n",
      "830 || Loss: 0.0247 || 10iter: 1.9743 sec.\n",
      "840 || Loss: 0.0338 || 10iter: 16.1262 sec.\n",
      "850 || Loss: 0.0452 || 10iter: 16.1186 sec.\n",
      "860 || Loss: 0.0308 || 10iter: 16.1404 sec.\n",
      "870 || Loss: 0.0314 || 10iter: 16.1251 sec.\n",
      "880 || Loss: 0.0290 || 10iter: 16.1621 sec.\n",
      "890 || Loss: 0.0436 || 10iter: 16.2048 sec.\n",
      "900 || Loss: 0.0369 || 10iter: 16.0915 sec.\n",
      "910 || Loss: 0.0361 || 10iter: 16.1187 sec.\n",
      "920 || Loss: 0.0198 || 10iter: 16.0395 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 10 || Epoch_TRAIN_Loss:0.0284 ||Epoch_VAL_Loss:0.0403\n",
      "timer:  203.3697 sec.\n",
      "-------------\n",
      "Epoch 11/30\n",
      "-------------\n",
      "（train）\n",
      "930 || Loss: 0.0266 || 10iter: 16.1675 sec.\n",
      "940 || Loss: 0.0192 || 10iter: 16.1062 sec.\n",
      "950 || Loss: 0.0255 || 10iter: 16.1272 sec.\n",
      "960 || Loss: 0.0425 || 10iter: 16.1318 sec.\n",
      "970 || Loss: 0.0231 || 10iter: 16.1215 sec.\n",
      "980 || Loss: 0.0260 || 10iter: 16.1365 sec.\n",
      "990 || Loss: 0.0387 || 10iter: 16.1219 sec.\n",
      "1000 || Loss: 0.0274 || 10iter: 16.1470 sec.\n",
      "1010 || Loss: 0.0260 || 10iter: 16.1029 sec.\n",
      "-------------\n",
      "epoch 11 || Epoch_TRAIN_Loss:0.0276 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.9124 sec.\n",
      "-------------\n",
      "Epoch 12/30\n",
      "-------------\n",
      "（train）\n",
      "1020 || Loss: 0.0235 || 10iter: 12.5694 sec.\n",
      "1030 || Loss: 0.0330 || 10iter: 16.1183 sec.\n",
      "1040 || Loss: 0.0226 || 10iter: 16.1264 sec.\n",
      "1050 || Loss: 0.0326 || 10iter: 16.1359 sec.\n",
      "1060 || Loss: 0.0199 || 10iter: 16.1389 sec.\n",
      "1070 || Loss: 0.0249 || 10iter: 16.1404 sec.\n",
      "1080 || Loss: 0.0280 || 10iter: 16.1229 sec.\n",
      "1090 || Loss: 0.0198 || 10iter: 16.1260 sec.\n",
      "1100 || Loss: 0.0413 || 10iter: 16.1561 sec.\n",
      "-------------\n",
      "epoch 12 || Epoch_TRAIN_Loss:0.0265 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.8836 sec.\n",
      "-------------\n",
      "Epoch 13/30\n",
      "-------------\n",
      "（train）\n",
      "1110 || Loss: 0.0207 || 10iter: 9.0342 sec.\n",
      "1120 || Loss: 0.0231 || 10iter: 16.1301 sec.\n",
      "1130 || Loss: 0.0248 || 10iter: 16.1151 sec.\n",
      "1140 || Loss: 0.0297 || 10iter: 16.1246 sec.\n",
      "1150 || Loss: 0.0299 || 10iter: 16.1258 sec.\n",
      "1160 || Loss: 0.0231 || 10iter: 16.1335 sec.\n",
      "1170 || Loss: 0.0279 || 10iter: 16.1133 sec.\n",
      "1180 || Loss: 0.0206 || 10iter: 16.1250 sec.\n",
      "1190 || Loss: 0.0434 || 10iter: 16.1266 sec.\n",
      "-------------\n",
      "epoch 13 || Epoch_TRAIN_Loss:0.0262 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.8408 sec.\n",
      "-------------\n",
      "Epoch 14/30\n",
      "-------------\n",
      "（train）\n",
      "1200 || Loss: 0.0213 || 10iter: 5.5032 sec.\n",
      "1210 || Loss: 0.0163 || 10iter: 16.1268 sec.\n",
      "1220 || Loss: 0.0234 || 10iter: 16.1415 sec.\n",
      "1230 || Loss: 0.0276 || 10iter: 16.1509 sec.\n",
      "1240 || Loss: 0.0176 || 10iter: 16.1165 sec.\n",
      "1250 || Loss: 0.0205 || 10iter: 16.1248 sec.\n",
      "1260 || Loss: 0.0301 || 10iter: 16.1358 sec.\n",
      "1270 || Loss: 0.0327 || 10iter: 16.1325 sec.\n",
      "1280 || Loss: 0.0258 || 10iter: 16.1208 sec.\n",
      "-------------\n",
      "epoch 14 || Epoch_TRAIN_Loss:0.0263 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.8998 sec.\n",
      "-------------\n",
      "Epoch 15/30\n",
      "-------------\n",
      "（train）\n",
      "1290 || Loss: 0.0201 || 10iter: 1.9805 sec.\n",
      "1300 || Loss: 0.0159 || 10iter: 16.1207 sec.\n",
      "1310 || Loss: 0.0168 || 10iter: 16.1506 sec.\n",
      "1320 || Loss: 0.0261 || 10iter: 16.1284 sec.\n",
      "1330 || Loss: 0.0219 || 10iter: 16.1182 sec.\n",
      "1340 || Loss: 0.0330 || 10iter: 16.1406 sec.\n",
      "1350 || Loss: 0.0239 || 10iter: 16.1417 sec.\n",
      "1360 || Loss: 0.0241 || 10iter: 16.1116 sec.\n",
      "1370 || Loss: 0.0231 || 10iter: 16.1476 sec.\n",
      "1380 || Loss: 0.0269 || 10iter: 16.0404 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 15 || Epoch_TRAIN_Loss:0.0259 ||Epoch_VAL_Loss:0.0382\n",
      "timer:  203.3645 sec.\n",
      "-------------\n",
      "Epoch 16/30\n",
      "-------------\n",
      "（train）\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1390 || Loss: 0.0257 || 10iter: 16.1629 sec.\n",
      "1400 || Loss: 0.0179 || 10iter: 16.1350 sec.\n",
      "1410 || Loss: 0.0209 || 10iter: 16.1234 sec.\n",
      "1420 || Loss: 0.0213 || 10iter: 16.1583 sec.\n",
      "1430 || Loss: 0.0226 || 10iter: 16.1158 sec.\n",
      "1440 || Loss: 0.0303 || 10iter: 16.1088 sec.\n",
      "1450 || Loss: 0.0247 || 10iter: 16.1344 sec.\n",
      "1460 || Loss: 0.0191 || 10iter: 16.1330 sec.\n",
      "1470 || Loss: 0.0213 || 10iter: 16.1334 sec.\n",
      "-------------\n",
      "epoch 16 || Epoch_TRAIN_Loss:0.0245 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.9510 sec.\n",
      "-------------\n",
      "Epoch 17/30\n",
      "-------------\n",
      "（train）\n",
      "1480 || Loss: 0.0331 || 10iter: 12.5952 sec.\n",
      "1490 || Loss: 0.0195 || 10iter: 16.1378 sec.\n",
      "1500 || Loss: 0.0210 || 10iter: 16.1647 sec.\n",
      "1510 || Loss: 0.0258 || 10iter: 16.1246 sec.\n",
      "1520 || Loss: 0.0212 || 10iter: 16.1369 sec.\n",
      "1530 || Loss: 0.0311 || 10iter: 16.1379 sec.\n",
      "1540 || Loss: 0.0293 || 10iter: 16.1221 sec.\n",
      "1550 || Loss: 0.0177 || 10iter: 16.1113 sec.\n",
      "1560 || Loss: 0.0210 || 10iter: 16.1272 sec.\n",
      "-------------\n",
      "epoch 17 || Epoch_TRAIN_Loss:0.0246 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.9423 sec.\n",
      "-------------\n",
      "Epoch 18/30\n",
      "-------------\n",
      "（train）\n",
      "1570 || Loss: 0.0306 || 10iter: 9.0339 sec.\n",
      "1580 || Loss: 0.0192 || 10iter: 16.1265 sec.\n",
      "1590 || Loss: 0.0255 || 10iter: 16.1250 sec.\n",
      "1600 || Loss: 0.0238 || 10iter: 16.1036 sec.\n",
      "1610 || Loss: 0.0315 || 10iter: 16.1501 sec.\n",
      "1620 || Loss: 0.0208 || 10iter: 16.1472 sec.\n",
      "1630 || Loss: 0.0257 || 10iter: 16.1558 sec.\n",
      "1640 || Loss: 0.0298 || 10iter: 16.1491 sec.\n",
      "1650 || Loss: 0.0230 || 10iter: 16.1160 sec.\n",
      "-------------\n",
      "epoch 18 || Epoch_TRAIN_Loss:0.0238 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.9326 sec.\n",
      "-------------\n",
      "Epoch 19/30\n",
      "-------------\n",
      "（train）\n",
      "1660 || Loss: 0.0302 || 10iter: 5.5202 sec.\n",
      "1670 || Loss: 0.0214 || 10iter: 16.1434 sec.\n",
      "1680 || Loss: 0.0181 || 10iter: 16.1092 sec.\n",
      "1690 || Loss: 0.0264 || 10iter: 16.1314 sec.\n",
      "1700 || Loss: 0.0296 || 10iter: 16.1573 sec.\n",
      "1710 || Loss: 0.0146 || 10iter: 16.1364 sec.\n",
      "1720 || Loss: 0.0355 || 10iter: 16.1475 sec.\n",
      "1730 || Loss: 0.0229 || 10iter: 16.1309 sec.\n",
      "1740 || Loss: 0.0129 || 10iter: 16.1313 sec.\n",
      "-------------\n",
      "epoch 19 || Epoch_TRAIN_Loss:0.0237 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.9630 sec.\n",
      "-------------\n",
      "Epoch 20/30\n",
      "-------------\n",
      "（train）\n",
      "1750 || Loss: 0.0464 || 10iter: 1.9585 sec.\n",
      "1760 || Loss: 0.0195 || 10iter: 16.1367 sec.\n",
      "1770 || Loss: 0.0166 || 10iter: 16.1345 sec.\n",
      "1780 || Loss: 0.0389 || 10iter: 16.1333 sec.\n",
      "1790 || Loss: 0.0186 || 10iter: 16.1105 sec.\n",
      "1800 || Loss: 0.0196 || 10iter: 16.1353 sec.\n",
      "1810 || Loss: 0.0350 || 10iter: 16.1501 sec.\n",
      "1820 || Loss: 0.0204 || 10iter: 16.1591 sec.\n",
      "1830 || Loss: 0.0251 || 10iter: 16.1349 sec.\n",
      "1840 || Loss: 0.0251 || 10iter: 16.0514 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 20 || Epoch_TRAIN_Loss:0.0241 ||Epoch_VAL_Loss:0.0381\n",
      "timer:  203.3414 sec.\n",
      "-------------\n",
      "Epoch 21/30\n",
      "-------------\n",
      "（train）\n",
      "1850 || Loss: 0.0192 || 10iter: 16.1572 sec.\n",
      "1860 || Loss: 0.0183 || 10iter: 16.1279 sec.\n",
      "1870 || Loss: 0.0231 || 10iter: 16.1230 sec.\n",
      "1880 || Loss: 0.0192 || 10iter: 16.1306 sec.\n",
      "1890 || Loss: 0.0144 || 10iter: 16.1355 sec.\n",
      "1900 || Loss: 0.0362 || 10iter: 16.1330 sec.\n",
      "1910 || Loss: 0.0329 || 10iter: 16.1285 sec.\n",
      "1920 || Loss: 0.0228 || 10iter: 16.1413 sec.\n",
      "1930 || Loss: 0.0225 || 10iter: 16.1009 sec.\n",
      "-------------\n",
      "epoch 21 || Epoch_TRAIN_Loss:0.0240 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.9280 sec.\n",
      "-------------\n",
      "Epoch 22/30\n",
      "-------------\n",
      "（train）\n",
      "1940 || Loss: 0.0245 || 10iter: 12.5842 sec.\n",
      "1950 || Loss: 0.0210 || 10iter: 16.1220 sec.\n",
      "1960 || Loss: 0.0211 || 10iter: 16.1274 sec.\n",
      "1970 || Loss: 0.0237 || 10iter: 16.1272 sec.\n",
      "1980 || Loss: 0.0271 || 10iter: 16.1006 sec.\n",
      "1990 || Loss: 0.0250 || 10iter: 16.1374 sec.\n",
      "2000 || Loss: 0.0203 || 10iter: 16.1424 sec.\n",
      "2010 || Loss: 0.0288 || 10iter: 16.1242 sec.\n",
      "2020 || Loss: 0.0283 || 10iter: 16.1514 sec.\n",
      "-------------\n",
      "epoch 22 || Epoch_TRAIN_Loss:0.0239 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.9222 sec.\n",
      "-------------\n",
      "Epoch 23/30\n",
      "-------------\n",
      "（train）\n",
      "2030 || Loss: 0.0241 || 10iter: 9.0553 sec.\n",
      "2040 || Loss: 0.0202 || 10iter: 16.1136 sec.\n",
      "2050 || Loss: 0.0185 || 10iter: 16.1111 sec.\n",
      "2060 || Loss: 0.0178 || 10iter: 16.1228 sec.\n",
      "2070 || Loss: 0.0459 || 10iter: 16.1365 sec.\n",
      "2080 || Loss: 0.0265 || 10iter: 16.1231 sec.\n",
      "2090 || Loss: 0.0186 || 10iter: 16.1369 sec.\n",
      "2100 || Loss: 0.0263 || 10iter: 16.1411 sec.\n",
      "2110 || Loss: 0.0246 || 10iter: 16.2714 sec.\n",
      "-------------\n",
      "epoch 23 || Epoch_TRAIN_Loss:0.0237 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  162.0690 sec.\n",
      "-------------\n",
      "Epoch 24/30\n",
      "-------------\n",
      "（train）\n",
      "2120 || Loss: 0.0207 || 10iter: 5.5028 sec.\n",
      "2130 || Loss: 0.0188 || 10iter: 16.1088 sec.\n",
      "2140 || Loss: 0.0217 || 10iter: 16.1394 sec.\n",
      "2150 || Loss: 0.0396 || 10iter: 16.1393 sec.\n",
      "2160 || Loss: 0.0127 || 10iter: 16.1415 sec.\n",
      "2170 || Loss: 0.0158 || 10iter: 16.1325 sec.\n",
      "2180 || Loss: 0.0128 || 10iter: 16.1265 sec.\n",
      "2190 || Loss: 0.0343 || 10iter: 16.1214 sec.\n",
      "2200 || Loss: 0.0206 || 10iter: 16.1447 sec.\n",
      "-------------\n",
      "epoch 24 || Epoch_TRAIN_Loss:0.0237 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.9343 sec.\n",
      "-------------\n",
      "Epoch 25/30\n",
      "-------------\n",
      "（train）\n",
      "2210 || Loss: 0.0236 || 10iter: 1.9755 sec.\n",
      "2220 || Loss: 0.0279 || 10iter: 16.1226 sec.\n",
      "2230 || Loss: 0.0288 || 10iter: 16.1621 sec.\n",
      "2240 || Loss: 0.0196 || 10iter: 16.1123 sec.\n",
      "2250 || Loss: 0.0199 || 10iter: 16.1281 sec.\n",
      "2260 || Loss: 0.0151 || 10iter: 16.1541 sec.\n",
      "2270 || Loss: 0.0172 || 10iter: 16.1361 sec.\n",
      "2280 || Loss: 0.0221 || 10iter: 16.1496 sec.\n",
      "2290 || Loss: 0.0208 || 10iter: 16.1339 sec.\n",
      "2300 || Loss: 0.0214 || 10iter: 16.0543 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 25 || Epoch_TRAIN_Loss:0.0228 ||Epoch_VAL_Loss:0.0374\n",
      "timer:  203.3772 sec.\n",
      "-------------\n",
      "Epoch 26/30\n",
      "-------------\n",
      "（train）\n",
      "2310 || Loss: 0.0174 || 10iter: 16.1820 sec.\n",
      "2320 || Loss: 0.0156 || 10iter: 16.1385 sec.\n",
      "2330 || Loss: 0.0207 || 10iter: 16.1485 sec.\n",
      "2340 || Loss: 0.0209 || 10iter: 16.1397 sec.\n",
      "2350 || Loss: 0.0225 || 10iter: 16.1144 sec.\n",
      "2360 || Loss: 0.0291 || 10iter: 16.1377 sec.\n",
      "2370 || Loss: 0.0220 || 10iter: 16.1357 sec.\n",
      "2380 || Loss: 0.0214 || 10iter: 16.1137 sec.\n",
      "2390 || Loss: 0.0219 || 10iter: 16.1303 sec.\n",
      "-------------\n",
      "epoch 26 || Epoch_TRAIN_Loss:0.0224 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.9880 sec.\n",
      "-------------\n",
      "Epoch 27/30\n",
      "-------------\n",
      "（train）\n",
      "2400 || Loss: 0.0182 || 10iter: 12.6184 sec.\n",
      "2410 || Loss: 0.0191 || 10iter: 16.1347 sec.\n",
      "2420 || Loss: 0.0434 || 10iter: 16.1404 sec.\n",
      "2430 || Loss: 0.0229 || 10iter: 16.1185 sec.\n",
      "2440 || Loss: 0.0198 || 10iter: 16.1397 sec.\n",
      "2450 || Loss: 0.0184 || 10iter: 16.1416 sec.\n",
      "2460 || Loss: 0.0242 || 10iter: 16.1161 sec.\n",
      "2470 || Loss: 0.0232 || 10iter: 16.1339 sec.\n",
      "2480 || Loss: 0.0145 || 10iter: 16.1304 sec.\n",
      "-------------\n",
      "epoch 27 || Epoch_TRAIN_Loss:0.0225 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  161.9551 sec.\n",
      "-------------\n",
      "Epoch 28/30\n",
      "-------------\n",
      "（train）\n",
      "2490 || Loss: 0.0195 || 10iter: 9.0621 sec.\n",
      "2500 || Loss: 0.0280 || 10iter: 16.1753 sec.\n",
      "2510 || Loss: 0.0209 || 10iter: 16.1460 sec.\n",
      "2520 || Loss: 0.0171 || 10iter: 16.1623 sec.\n",
      "2530 || Loss: 0.0250 || 10iter: 16.1157 sec.\n",
      "2540 || Loss: 0.0269 || 10iter: 16.1601 sec.\n",
      "2550 || Loss: 0.0295 || 10iter: 16.1517 sec.\n",
      "2560 || Loss: 0.0234 || 10iter: 16.1299 sec.\n",
      "2570 || Loss: 0.0267 || 10iter: 16.2400 sec.\n",
      "-------------\n",
      "epoch 28 || Epoch_TRAIN_Loss:0.0226 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  162.1744 sec.\n",
      "-------------\n",
      "Epoch 29/30\n",
      "-------------\n",
      "（train）\n",
      "2580 || Loss: 0.0276 || 10iter: 5.7139 sec.\n",
      "2590 || Loss: 0.0195 || 10iter: 16.2134 sec.\n",
      "2600 || Loss: 0.0209 || 10iter: 16.1515 sec.\n",
      "2610 || Loss: 0.0204 || 10iter: 16.1496 sec.\n",
      "2620 || Loss: 0.0285 || 10iter: 16.2061 sec.\n",
      "2630 || Loss: 0.0295 || 10iter: 16.2214 sec.\n",
      "2640 || Loss: 0.0147 || 10iter: 16.1494 sec.\n",
      "2650 || Loss: 0.0239 || 10iter: 16.1396 sec.\n",
      "2660 || Loss: 0.0195 || 10iter: 16.1992 sec.\n",
      "-------------\n",
      "epoch 29 || Epoch_TRAIN_Loss:0.0226 ||Epoch_VAL_Loss:0.0000\n",
      "timer:  162.5470 sec.\n",
      "-------------\n",
      "Epoch 30/30\n",
      "-------------\n",
      "（train）\n",
      "2670 || Loss: 0.0251 || 10iter: 1.9771 sec.\n",
      "2680 || Loss: 0.0243 || 10iter: 16.3731 sec.\n",
      "2690 || Loss: 0.0202 || 10iter: 16.3127 sec.\n",
      "2700 || Loss: 0.0273 || 10iter: 16.2225 sec.\n",
      "2710 || Loss: 0.0142 || 10iter: 16.1555 sec.\n",
      "2720 || Loss: 0.0172 || 10iter: 16.1333 sec.\n",
      "2730 || Loss: 0.0199 || 10iter: 16.1623 sec.\n",
      "2740 || Loss: 0.0238 || 10iter: 16.1773 sec.\n",
      "2750 || Loss: 0.0262 || 10iter: 16.1440 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2760 || Loss: 0.0264 || 10iter: 16.0467 sec.\n",
      "-------------\n",
      "（val）\n",
      "-------------\n",
      "epoch 30 || Epoch_TRAIN_Loss:0.0230 ||Epoch_VAL_Loss:0.0373\n",
      "timer:  204.0129 sec.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3-7_PSPNet_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
